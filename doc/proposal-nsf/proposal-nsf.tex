%%%%%%%%% MASTER -- compiles the 4 sections

\documentclass[11pt,letterpaper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{plain}                                                      %%
%%%%%%%%%% EXACT 1in MARGINS %%%%%%%                                   %%
\setlength{\textwidth}{6.5in}     %%                                   %%
\setlength{\oddsidemargin}{0in}   %% (It is recommended that you       %%
\setlength{\evensidemargin}{0in}  %%  not change these parameters,     %%
\setlength{\textheight}{8.5in}    %%  at the risk of having your       %%
\setlength{\topmargin}{0in}       %%  proposal dismissed on the basis  %%
\setlength{\headheight}{0in}      %%  of incorrect formatting!!!)      %%
\setlength{\headsep}{0in}         %%                                   %%
\setlength{\footskip}{.5in}       %%                                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                   %%
\newcommand{\required}[1]{\section*{\hfil #1\hfil}}                    %%
\renewcommand{\refname}{\hfil References Cited\hfil}                   %%
\bibliographystyle{plain}                                              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage[T1]{fontenc}
% \usepackage[scaled]{uarial}
% \renewcommand*\familydefault{\sfdefault}

% \usepackage{setspace}
\usepackage{epsfig}
\usepackage{url}

\include{newcommand}
% INCLUDE DEVELOPMENT TEXT

% \newcommand{\devel}[1]{\textbf{#1}}
% 
% % EXCLUDE DEVELOPMENT TEXT
% 
% % \newcommand{\devel}[1]{}
% 
% %\newcommand{\SUBSUBSECTION}[1]{\subsubsection{#1} }
% \newcommand{\SUBSUBSECTION}[1]{\textbf{#1.}~}
% 
% \newcommand{\pargraph}[1]{\devel{\P\ \textbf{#1} \\}}
% \newcommand{\cello}{\textsf{Cello}}
% \newcommand{\enzo}{\textsf{Enzo}}
% %\newcommand{\enzoii}{\textsf{Enzo}-\texttt{II}}
% \newcommand{\enzoii}{\textsf{Enzo-P}}
% \newcommand{\lcaperf}{\textsf{lcaperf}}
% \newcommand{\lcatest}{\textsf{lcatest}}
% 
% \newcommand{\pp}{\texttt{++}}
% \newcommand{\cpp}{C\pp}
% \newcommand{\charm}{\textsf{Charm\pp}}
% \newcommand{\chombo}{\textsf{Chombo}}
% \newcommand{\samrai}{\textsf{SAMRAI}}
% \newcommand{\paramesh}{\textsf{PARAMESH}}
% \newcommand{\gadget}{\textsf{GADGET}}
% \newcommand{\alps}{\textsf{ALPS}}
% \newcommand{\clawpack}{\textsf{CLAWPACK}}
% \newcommand{\grace}{\textsf{GrACe}}
% \newcommand{\carpet}{\textsf{Carpet}}
% \newcommand{\flash}{\textsf{FLASH}}
% 
% \newcommand{\code}[1]{\textsf{#1}}
% 
% \newcounter{figctr}
% 
% \newcommand{\FIGURE}[3]{
% \noindent
% \parbox{\textwidth}{
% %\ \\ \hrule \ \\
% \begin{center}
% #3
% \end{center}%
% \ \nolinebreak%
% \refstepcounter{figctr}%
% \begin{center}%
% \begin{minipage}{\textwidth}
% \textbf{Figure \thefigctr}. #1
% \end{minipage}
% \end{center}
% \label{#2}
% %\ \\ \hrule \ \\
% }}

% NSF proposal generation template style file.
% based on latex stylefiles  written by Stefan Llewellyn Smith and
% Sarah Gille, with contributions from other collaborators.

\DeclareFontFamily{OT1}{psyr}{}
\DeclareFontShape{OT1}{psyr}{m}{n}{<-> psyr}{}
\def\times{{\fontfamily{psyr}\selectfont\char180}}


\renewcommand{\refname}{\centerline{References cited}}

% this handles hanging indents for publications
\def\rrr#1\\{\par
\medskip\hbox{\vbox{\parindent=2em\hsize=6.12in
\hangindent=4em\hangafter=1#1}}}

\def\baselinestretch{1}

% \setcounter{secnumdepth}{2}

%-----------------------------------------------------------------------
% NSF
%-----------------------------------------------------------------------
%
% FIVE SOFTWARE FOCUS AREAS
%
%  * 1. software for HPC systems
%    2. software for digital data management
%    3. software for broadband and networking
%    4. middleware
%    5. cybersecurity
%
% CROSS CUTTING ISSUES
%
%  * software sustainability
%
%  * software self-manageability
%
%  - software power/energy efficiency
%
%
% HPC SOFTWARE ISSUES
%
%  * deep-memory hierarchies
%  * multi-core architectures
%  * heterogeneous/hybrid systems
%  * architecture agnostic
%
% SDCI REQUIREMENTS
%
%  * identify software focus area and category in title
%  * support for at least one cross-cutting issue
%  * identify multiple application areas in science or engineering
%    - missing capability required
%    - specific examples of how tool will impact science research
%  * clear description of how approach compares to existing approaches
%  * explicit outreach and education plan for additional end user groups
%  * explicit description of the engineering process used
%    - design, development, release, deployments, tool inter-operability,
%    - evaluation plan that includes end users [ref nmi.cs.wisc.edu]
%  * list of tangible metrics to measure success, with end users
%    - quantitative + qualitative definition of "working prototype"
%    - steps from prototype to production use
%  * compelling discussion of software's potential use by broader communities
%    - use cases with relevant domain scientists
%  * sustainability plan beyond the award lifetime
%  * identify open source licence
%    
%-----------------------------------------------------------------------


% Project summary
% Introduction
%    Extreme parallelism
%    Extreme AMR
%    Existing AMR frameworks
%       PARAMESH
%       Chombo
%       SAMRAI
%       GADGET
% Software requirements
% Software design
%    High level components
%    Data structures
%       Patch coalescing  for ``shallow'' AMR
%       Targeted refinement with backfill for ``deep'' AMR
%    Task scheduling
%    Load balancing
% Implementation
%    Parallelism
%    Fault tolerance
%    Software implementation
% Development plan
% Milestones and deliverables

\usepackage{natbib}

\newcommand{\delete}[1]{}

%=======================================================================

\begin{document}

% \tableofcontents
%\Large

% \nocite{StSh09} % Scalability challenges for massively parallel {AMR} applications
% \nocite{WiHy03} % Enhancing scalability of parallel structured {AMR} calculations
%\nocite{GuWi06} % Parallel clustering algorithms for structured {AMR}
% \nocite{BuGh08} % Towards adaptive mesh {PDE} simulations on petascale computers
% \nocite{BaBu09} % MPI on a million processors

%\nocite{LaTa06} % hierarchical load balancing

\begin{center}

%{\Large{\bf SDCI HPC New: The \cello\ Software Framework for \\ Extreme Adaptive Mesh
%  Refinement}}\\*[3mm]
%James Bordner \\Michael L. Norman \\Robert Harkness \\Alexei Kritsuk \\

\end{center}

%=======================================================================
%\TITLE{}{James Bordner}{$Rev$}
%=======================================================================

%
%
%

% Keywords
%
%    AMR
%    applied mathematics
%    collaborative software environments
%    computer science technologies
%    exascale
%    extreme scale
%    failure avoidance
%    failure effects avoidance
%    fault tolerance
%    GPU
%    hierarchical
%    HPC
%    memory hierarchy
%    multicore
%    multiphysics
%    multiscale
%    one-sided communication
%    performance tuning
%    petascale
%    PGAS
%    research goals
%    resilient software
%    UPC
%    virtual processes
%
% suggestions
%
%    preliminary results
%    good references critical; only give great references
%    use rfp keywoards
%    sustainability--what happens when money ends
%    sales document not technical manual
%    visit agency web site for similar funding
%    make benefits clear
%    write for evaluators
%    justify and support all claims
%    active voice: strong subjects and active verbs
%    first paragraph sentence conveys topic
%    65 characters per line
%    duplicate information instead of cross reference
%    use graphics and tables
%    short text with bullets
%    stay on topic
%
% more suggestions
%
% [ compelling, organization goals and success, relevance to agency]
% problem statement--needs assessment
%     motivation / history
%       enzo
%       cello project
%       decoupled framework
%       well-aware of scalability issues
%       understand importance of performance / scalability of all components
%     purpose
%     beneficiaries
%     what is being done
%     what we will do
% objectives: table with direct items
% development plan
%    chart with timelines
%    decision points
%    milestones
%    tasks
% methods and design
%     use flow charts and diagrams to break up text
%     justify course of action
%     highlight innovative features
% past experience
% management approach
%   quality control
%   emphasize performance management
%   technological systems in place


%=======================================================================
\section{Introduction} \label{s:intro}
%=======================================================================

% \cite{StSh09} quote
% load balancing and communication are often cited as the main
% barriers to AMR scaling, but instead ``...many of the scaling problems related to early design decisions''

% \pargraph{LCA community code history}
% \begin{itemize}
% \item history of community codes
% \item large user base
% \item propose \enzoii\ + \cello
% \begin{itemize}
% \item retains existing cosmology astrophysics users
% \item adds users of multiresolution multiphysics: [list]
% \end{itemize}
% \end{itemize}

% \pargraph{MIKE: LCA history}


\SUBSUBSECTION{History and motivation} The parallel astrophysics and
cosmology application \enzo~\cite{OsBr04} was conceived in the early
1990's by Michael L.~Norman and Greg Bryan, and implemented using
structured adaptive mesh refinement (SAMR)~\cite{BeCo89}.  It
incorporates a modified high-order Piecewise Parabolic Method (PPM)
solver for hydrodynamics~\cite{WoCo84b}, and a Particle-Mesh (PM)
method for dark matter dynamics~\cite{HoEa88}.  So far in its $>$15
year lifetime, \enzo\ simulations have led to important scientific
contributions to the fields of astrophysics and cosmology~[@@@ MIKE:
Enzo science references].  The success of \enzo\ can be attributed to
its AMR capabilities, range of physics options, and long-term
development and support commitment by the Laboratory for Computational
Astrophysics (LCA).

\enzo's AMR data structures scale well to around $10^4$ or $10^5$
processes, but efforts to improve its scalability beyond this have
become increasingly difficult due to the invasiveness of the desired
changes and complexity of the code base.  The \cello\ project
originated as an effort to redesign the parallel data structures of
``\enzo'' to create a highly scalable version ``\enzoii.''  \enzoii\
will be developed by combining the existing \enzo\ core physics
functions, which are continually being developed and improved, with
the underlying \cello\ extreme AMR framework.

By decoupling \enzoii's development into separate software
products---the underlying \cello\ extreme AMR framework and the
\enzoii\ parallel astrophysics application---we greatly increase the
usefulness of the resulting software.  [@@@ MIKE: scientific
motivation for extreme AMR]

\SUBSUBSECTION{Roadblocks to scalability} There are countless issues
with developing scalable parallel scientific frameworks and
applications, even at modest levels of scalability.  These issues
increase in number and become more crucial as scalability is pushed
from terascale through petascale and into exascale.  Many of these
scaling issues are known or suspected, such as
%
increased expected fault rates,
%
a breakdown in the disk checkpoint/restart paradigm due to disk
performance trends,
%
% increased importance of scaling of underlying algorithms and
% implementations,
%
increased importance of locality,
%
increased cost of global communication and synchronization,
%
% increased importance of uncovering and capitalizing on existing
% parallelism in the problem,
%
increased effects of variation in performance due to e.g.~operating
system jitter or memory management~\cite{StSh09},
%
utilization of heterogeneous/hybrid systems such as those including
GPGPU's,
%
and adapting to the inherently hierarchical nature of concurrent HPC
platforms.  Further unknown and unexpected issues are likely to arise
as well.


\SUBSUBSECTION{AMR scalability issues} AMR in particular has numerous additional
known and suspected issues with scaling, such as
%
AMR patch sizes restricting parallel task sizes,
%
keeping the dynamically changing workload continually balanced across
the HPC platform,
%
maintaining data locality between hardware nodes to control
communication costs,
%
maintaining data locality within nodes to control memory hierarchy
efficiency,
%
maintaining performance and scalability in remeshing and other AMR
operations,
%
solving inherently global elliptic problems efficiently,
%
and addressing increased range and precision requirements for global
indices and global positions.
%
Again, further unknown and unexpected issues are likely to arise.

\SUBSUBSECTION{Modus operandi} We will address scalability issues
using a wide range of general guidelines and specific algorithms,
including using existing proven techniques, refining existing
approaches that have shown promise, and developing new methods to
address remaining unresolved issues.  A guiding general principle will
be localization: we will aggressively remove all unnecessary global
communication, global synchronization, global indexing, and global
coordinate systems.  Some specific approaches that we will pursue
include the following:
%
1) using a hybrid replicated/distributed octree-based AMR approach, with
modifications to improve scaling in terms of both size (number of AMR
blocks) and depth (number of AMR levels);
% 
2) using patch-local adaptive time steps;
% 
3) supporting flexible hybrid parallelization strategies;
% 
4) using a hierarchical load balancing approach based on actual
performance measurements;
% 
5) dynamically scheduling tasks and communication;
% 
6) allowing flexible reorganization of AMR data in memory to permit
independent optimization of computation, communication, and storage;
% 
7) supporting variable AMR grid block sizes while still maintaining
bounded or fixed parallel task sizes;
% 
8) addressing the limited precision and range issues that arise when
pushing the range of spacial and temporal resolution ranges;
% 
and 9) detecting and handling hardware or software faults during
run-time to improve software resilience and enable software
self-management.

% Below in \S\ref{s:review} we review some of the existing software
% frameworks and applications that are most similar to our proposed
% effort.  Next we summarize our software design in \S\ref{s:design}, software
% development approach in \S\ref{s:implementation}, and software testing in
% \S\ref{s:testing}.  Finally we describe our development plan in
% \S\ref{s:plan}, and list our milestones and deliverables in
% \S\ref{s:milestones}.

%-----------------------------------------------------------------------
\section{Project Team and Institutional Commitment}
%-----------------------------------------------------------------------

The team of Laboratory for Computational Astrophysics computational
science experts---with overlapping yet targeted areas of expertise in
parallel software development, high performance computing, and
computational astrophysics---form the ideal group to design,
implement, and rigorously test the proposed software.
%
\textit{Bordner} Bordner has 20 years experience in parallel software development,
including distributed memory and shared memory programming, and object-oriented
parallel software design.
Bordner will be organizing the project, and will be the principle developer
of the \cello\ framework.
\textit{Kritsuk} [@@@ KRITSUK background] Kritsuk will be the principle
developer of the \enzoii\ application built on top of the \cello\ framework.
%
\textit{Harkness} [@@@ HARKNESS background] [@@@ HARKNESS tasks]
%
\textit{Norman} [@@@ NORMAN ]


The institutions involved, University of California San Diego, and 
the San Diego Supercomputer Center, are supportive of this project.

[@@@ more]


%-----------------------------------------------------------------------
%\include{section-facilities}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\section{Review of related software} \label{s:review}
%-----------------------------------------------------------------------

There are numerous AMR frameworks, libraries, and applications, each
with different design goals and decisions, data structures, AMR
algorithms, parallelization strategies, and resulting parallel
performance and scaling characteristics.  Perhaps the most closely
related software frameworks to our proposed project are
\samrai~\cite{WiHo01}~\cite{wwwsamraicode},
\chombo~\cite{wwwchombo}~\cite{CoGr09},
\paramesh~\cite{MaOl00}~\cite{OlMa05}~\cite{Ol06}~\cite{wwwparamesh},
\alps~\cite{BuBu09}, and \gadget~\cite{wwwgadget}~\cite{Sp05}, which
we discuss below.  Other notable frameworks include
\clawpack~\cite{wwwclawpack}, \grace~\cite{PaLi10}, and
\carpet~\cite{ScDi06}~\cite{wwwcarpet}.  

No existing AMR framework or application combines the software
resiliency, support for hybrid fluid + particle methods, extreme
parallel scaling, and support for extreme AMR breadth and depth, that
are required for new cutting-edge astrophysics science discoveries, on
current let alone future high-end NSF HPC platforms.  Nevertheless,
each software effort reviewed below has its strong points, and much
can be learned by studying the design decisions made and the resulting
software capabilities.

%-----------------------------------------------------------------------

\SUBSUBSECTION{\samrai}
%
\samrai~\cite{WiHo01}~\cite{wwwsamraicode} (Structured Adaptive Mesh
Refinement Application Infrastructure), is an MPI-parallel patch-based
structured AMR (SAMR) framework developed by the Center for Applied
Scientific Computing at Lawrence Livermore National Laboratory.  A
distinguishing capability of \samrai\ is its coupling of ALE
(Arbitrary Lagrangian-Eulerian) methods with AMR, which are especially
efficient and accurate for large-scale multi-material calculations.
The SAMR hierarchy uses sophisticated clustering algorithms for
defining the configuration of patches~\cite{GuWi06}.  Load balancing
is level-independent, and uses a Morton space-filling curve algorithm
to determine the patch distribution across processors~\cite{WiHo01}.
Partitioning the linearized set of patches across processors can be
based either on a spacially uniform work distribution, or based on a
user-specified non-uniform workload~\cite{wwwsamraicode}.  \samrai\
has demonstrated modest overall scaling of up to 1024
processors~\cite{WiHy03} in 2003, and algorithmic improvements to the
parallel clustering algorithm have improved the scaling of that
particular component on up to 16384 processors~\cite{GuWi06}.  While
it is a very powerful framework, especially for problems well-suited
to the ALE-AMR class of methods, issues with scaling indicate that
SAMR may not be the ideal choice of AMR data structure to meet our
extreme scaling requirements.

%-----------------------------------------------------------------------

\SUBSUBSECTION{\chombo} 
%
\chombo~\cite{wwwchombo}~\cite{CoGr09} is an MPI-parallel patch-based
SAMR framework in active development by the Applied Numerical
Algorithms Group of Lawrence Berkeley National Lab.  It supports
methods for solving coupled hyperbolic, elliptic, and parabolic PDE's.
\chombo\ is quite scalable, and designed to run both hyperbolic and
elliptic problems on 10,000 processors.  Both \cpp/Fortran and
Titanium~\cite{wwwtitanium}~\cite{YeSe98} versions have been
developed, and an effort to develop a UPC version was
undertaken~\cite{We04}.  Load balancing is performed using the
Kernighan-Lin algorithm within each level in the \cpp/Fortran
implementation, and using a Morton space-filling curve approach in the
Titanium implementation~\cite{WeSu07}.  \chombo\ also allows the user
to provide their own load balancing algorithm.  Scaling of
applications using \chombo\ have been good, with demonstrated scaling
of $87\%$ over $256$-$8192$ processors for elliptic
PDE's~\cite{CoBe07}.  Scaling and performance optimizations have
included reducing communication costs by using space-filling curves
for efficient load balancing, using fast search algorithms and local
caching for computing and storing block intersection lists, and
improving boundary condition computations at resolution level
boundaries.  All processes store the AMR metadata, which includes all
patch extents and process assignments.  Although the size has been
aggressively reduced to an impressive $\approx 50$ bytes per
patch~\cite{CoBe07}, this still represents an eventual hard scaling
limit for \chombo.  We agree with their assessment in~\cite{WeSu07}
that relaxing the bulk synchronous communication constraint, in favor
of smaller messages with one-sided communication and
communication-computation overlap, would further improve scalability;
we plan to pursue that approach from the start.

%-----------------------------------------------------------------------

\SUBSUBSECTION{\paramesh}
%
\paramesh~\cite{MaOl00}~\cite{OlMa05}~\cite{Ol06}~\cite{wwwparamesh}
is an octree-based AMR framework developed by the NASA Goddard Space
Flight Center and Drexel University.  \paramesh\ has been successfully
used in many applications, including serving as the AMR framework for
the \flash\ code for astrophysical thermonuclear
flashes~\cite{FrOl00}~\cite{wwwflash}.  \paramesh\ consists of a
collection of Fortran 90 subroutines, and is parallelized using the
MPI library.
%
In \paramesh, nodes of the octree are associated with small grid
blocks of fixed size.  Ghost zones may be permanently allocated or
not, resolution jumps are restricted to at most one level, and time
stepping may be constant or adaptive on different refinement levels.
Load balancing uses a linearized Morton or Peano-Hilbert space-filling
curve, with the option of assigning different weighting factors to
different blocks.  Checkpoint/restart to disk is implemented as well,
using either parallel HDF5~\cite{hdf5} or MPI-IO.  Applications
developed with \paramesh\ have exhibited good scaling, such as \flash\
which scales to $10,000$ processors~\cite{Du09}.
%
%
\paramesh\ has many attractive features, including associating small
grid blocks to octree nodes (as opposed to using a single
computational element or cell) to help amortize AMR overhead costs,
and supporting adaptive time steps with non-constant load balancing
weighting factors for blocks in different levels.  However, it does
not support particle methods.  Its fixed AMR block size is also a
potential performance issue---in particular, regions of fine but
uniform refinement will necessarily be covered by many small blocks,
rather than just a few larger patches as in the SAMR approach.
\paramesh\ also replicates AMR data on all processors, which
ultimately limits its scalability.

%-----------------------------------------------------------------------

\SUBSUBSECTION{\alps}
%
\alps~\cite{BuBu09}, developed at the University of Texas at Austin,
is a highly scalable AMR framework based on octrees.  It appears to be
one of the most highly scalable AMR frameworks currently available,
with demonstrated scaling on up to 32K cores and 4 billion
elements~\cite{BuGh08}.
%
%The framework provides a small collection of AMR-related functions,
%including functions for marking elements for coarsening/refinement,
%performing the remeshing step, balancing the tree to eliminate level
%jumps, interpolating field data after a remeshing step, and
%partitioning elements across cores.
The AMR hierarchy is octree-based, with only the leaves of the octree
used as elements, which can reduce communication since no
communication is required between levels~\cite{BuGh08b}.  The octree
is kept balanced to limit level jumps to at most one level difference,
and the hierarchy is fully distributed across cores using a linearized
Morton space-filling curve.  While solving elliptic problems is not
directly supported by the framework, it has been used with the
\code{BoomerAMG} algebraic multigrid solver~\cite{HeYa02}, which is
available in the \code{hypre} scalable linear solver
package~\cite{FaJo06}.
%
%The octree-based AMR approach has numerous features that could satisfy
%many of our requirements, including relatively high AMR data structure
%performance and scalability, a built-in spacial data structure that
%could support fast particle methods as well as mesh-based methods, and
%an established and effective parallel partitioning approach using
%space-filling curves.
%
While very scalable, \alps\ does not support particle methods, and
does not support adaptive time stepping required for scaling in terms
of AMR hierarchy depth.
%(due in part
%to its assigning an equal number of elements to each
%core~\cite{BuGh08b}).  
Also, as with \paramesh\ and other standard octree-based AMR
approaches, ``unigrid efficiency'' is lost in localized regions of
fine but uniform resolution.

%-----------------------------------------------------------------------

\SUBSUBSECTION{\gadget}
%
\gadget~\cite{wwwgadget}~\cite{Sp05}, developed by Volker Springel at
the Max-Planck-Institute for Astrophysics since 2000, is an
MPI-parallel cosmology application for massively parallel TreeSPH
simulations.  It supports long-range force calculations using an
FFT-based particle-mesh (PM) method, and octree-based hierarchical
multipole expansion for short-range forces.
%  The SPH formulation is
%entropy-conserving.
%
Particles are partitioned in \gadget-2.0 using the Barnes-Hut
algorithm on a distributed octree~\cite{BaHu86}, and octree nodes are
distributed using a Peano-Hilbert space-filling curve.  Particles are
integrated in time using adaptive time steps, where time steps are
allowed to differ from each other by factors of two.
%
While floating point accuracies can be an issue at the particle level,
collective statistical properties still converge.  In \cite{OsNa05} it
was shown that \gadget\ had better performance and lower memory usage
than the comparable gravity solver in \enzo\ at comparable force
resolution.  This is due to \enzo's hierarchical PM method, which
requires relatively fine mesh refinement to obtain comparable accuracy
of the short-range forces.
%
We wish to support an analogous PPPM method in \enzoii, so the \cello\
framework will similarly require a fast distributed spacial data
structure to quickly determine short-range versus long-range forces.

% %=======================================================================
% \section{Software Requirements} \label{s:require}
% %=======================================================================
% 
% 
% \cello\ will be a framework that allows application developers to
% write multiphysics PDE applications that span a wide range of
% resolutions, and that may require an amount of computational
% resources, memory, and storage only available on the largest available
% HPC platforms.  All software in the system will be released as open
% source.  Below we describe a subset of the software requirements
% specification, though more qualitatively than quantitatively.
% 
% Problems that the framework will support include systems of nonlinear
% PDE's, including hyperbolic conservation laws and elliptic equations.
% Specifically, in terms of the astrophysics/cosmological application
% \enzoii\ driving its development, the framework will support solving
% Euler's equations of hydrodynamics, magnetohydrodynamics,
% radiation-hydrodynamics, and Poisson's equation for self-gravity.
% 
% Spacial dimensions will include $3$D, but the framework will also
% support $1$D and $2$D, primarily for test problems.  Supported
% numerical methods, which are not in the scope of the framework but
% rather the user functions, include Eulerian mesh-based methods such as
% PPM~\cite{WoCo84b}, Lagrangian particle-based methods, and hybrid
% particle-mesh methods such as PM and PPPM~\cite{HoEa88}.
% 
% Optionally, we may also support multiple simulations run
% simultaneously; this could allow for inter-simulation inline analysis
% for parameter studies that would otherwise be infeasible due to
% excessive data storage requirements.
% 
% 
% %-----------------------------------------------------------------------
% \subsection{Adaptive mesh refinement} \label{ss:require-amr}
% %-----------------------------------------------------------------------
% 
% The AMR data structure must be highly scalable in both breadth and
% depth, supporting at least  $10^8$ grid blocks or patches and 
% $60$ levels of refinement.  The huge number of possible grid blocks
% indicates that the data structure must be at least partially
% distributed.  The extreme possible depth indicates that there must be
% no precision or range issues with global floating point positions or
% global integer indices.  The large range in resolutions also suggests
% that the framework should support flexible numerical scaling at
% different mesh levels, to reduce floating point errors and prevent
% underflow or overflow errors.
% 
% The AMR data structure should also be ``efficient'' at adapting in
% terms of the number of grid patches.  In particular, if much of the
% region consists of large areas of refined but uniform resolution, then
% the number of patches covering those regions should not be excessive.
% We note that this requirement essentially eliminates fixed-size
% patches, which most current octree-based AMR approaches use.
% 
% The AMR data structure should also be efficient and scalable in terms
% of all of its operations, including coarsening, refining, and locating
% neighboring patches.  Note that this indicates that some sort of
% spacial data structure or other optimization be used, such as octrees,
% chaining meshes, or caching intersection lists~\cite{StSh09}.
% 
% The AMR hierarchy should dynamically adapt to problem features as
% specified by the user.  Localized changes in resolution should remain
% bounded, e.g.~adjacent patches should be within one refinement
% level of each other, to maintain numerical accuracy.  Also, refinement
% of symmetric problems should retain logical symmetry (cells but not
% necessarily patches) in the AMR data structure, to help prevent
% asymmetries in the numerical solution.
% 
% 
% %-----------------------------------------------------------------------
% \subsubsection{Mesh data} \label{sss:require-fields}
% %-----------------------------------------------------------------------
% 
% Multiresolution data fields defined on logically Cartesian grids,
% used for Eulerian mesh-based and hybrid particle-mesh methods, will be
% supported by the AMR data structure.  The number and identification of
% fields will be flexible and declared by the user: no assumptions will
% be made on specific data field names or properties in the framework.
% 
% Stencil-based discretization methods will of course be supported,
% which indicates that a layer of ``ghost'' or ``guard'' cells will be
% required for computations.  The framework will allow flexible layers
% of ghost zones, up to at least three zones deep.  Furthermore, the
% number and depth of ghost zones may depend on the specific field, and
% ghost zones may or may not be permanently stored to conserve physical
% memory.
% 
% Data fields may be single or double precision, and different fields
% may have different precisions, depending on the relative importance of
% storage and accuracy/range for individual fields.  The framework will
% also support scaling different fields by different amounts at
% different levels, to improve the numerical behavior of user methods on
% an extreme range of scales.
% 
% The framework will also allow for user methods to operate on
% individual patches, all patches within a level, or all patches in the
% hierarchy.  The motivation is that the most scalable and accurate
% methods for coupled hyperbolic and elliptic problems defined on AMR
% hierarchies typically involve solving linear systems defined both
% within a single level and on the entire AMR hierarchy~\cite{MiCo07}.
% 
% 
% %-----------------------------------------------------------------------
% \subsubsection{Particle data} \label{sss:require-particles}
% %-----------------------------------------------------------------------
% 
% In addition to mesh data, particle data will also be supported, for
% use in Lagrangian particle methods and hybrid particle-mesh methods.
% Multiple groups of particles will be supported, e.g.~\enzoii\ will
% include ``star'' particles, dark matter particles for collisionless
% gravitational dynamics, tracer particles for analysis and
% visualization, etc.  All particles will have position, and different
% particle groups will have different user-defined attributes associated
% with them, such as velocity, mass or momentum, etc.
% 
% Extreme particle counts will be supported, indicating that particles
% must be at least partially distributed.  Since many algorithms on
% particles require particle proximities, operations such as finding
% nearest neighbors must be efficient.  This indicates that an efficient
% distributed spacial data structure must be used, such as an octree or
% orthogonal recursive bisection.  Also, since hybrid particle-mesh
% methods will be supported, efficient operations must be implemented
% for locating all particles within a patch, and for locating the block
% or patch overlapping a given particle.
% 
% %-----------------------------------------------------------------------
% \subsection{Parallelism} \label{ss:require-parallel}
% %-----------------------------------------------------------------------
% 
% The framework will be highly scalable, so parallelism is a key feature
% of the software.  There will be no fundamental limit on process or
% thread counts, i.e.~no hard-coded \code{MAX\_PROCESSORS}-type
% constant.
% 
% Parallelism will include both distributed and shared memory models, as
% well as hybrid parallelism.  This implies that many sections of both
% the framework and user code must be thread safe.  The distributed
% memory model is necessary for running efficiently on platforms that do
% not support a single virtual memory address space, and the shared
% memory model can help reduce memory use by allowing shared data
% between threads instead of duplicating data between processes.
% 
% Parallel task definition is crucial to both performance and scaling of
% parallel applications---there must be enough tasks to provide
% sufficient parallelism, individual tasks must be large enough that
% overhead does not dominate computation, and tasks must be defined such
% that there is not excessive inter-task communication.  We note that
% this requirement can conflict with standard AMR data structures if
% individual AMR patches are taken to be individual parallel tasks.
% %This is because patch counts and sizes are dependent functions of
% %resolution requirements, which leaves parallelism to be only
% %indirectly controllable through AMR parameters, such as limits on the
% %range of allowable patch sizes.  
% This indicates that relaxing the constraint of defining parallel tasks
% as individual AMR patches could be beneficial.
% 
% The framework will directly support multiple levels of parallelism,
% not just at the distributed memory / shared memory levels.
% Specifically, the framework will support up to four levels of
% parallelism, for example corresponding to hardware cabinets, nodes,
% processors, and cores.  We expect that this will improve both
% efficiency and scalability, by allowing more flexible mapping of the
% distributed data structures onto the hierarchical parallel hardware,
% and by reducing the maximum size of groups of intercommunicating
% processes~\cite{BaBu09}.
% 
% Task scheduling must also be efficient, minimizing overhead and
% maximizing throughput.  Again, a hierarchical scheduling approach may
% be used.  Since we want to at least indirectly support GPGPU
% programming in user code, we will want to support scheduling large
% groups of concurrent tasks to keep the GPGPU's busy.
% 
% %-----------------------------------------------------------------------
% \subsubsection{Load balancing} \label{ss:require-balance}
% %-----------------------------------------------------------------------
% 
% Dynamic load balancing / task migration is critical for parallel
% performance of AMR, due to the dynamically changing workload
% distributed across nodes with limited memory and potentially millions
% of computational cores.  Dynamic load balancing must therefore be
% scalable, efficient, reliable, and effective.  Efficiency indicates
% that data locality is maintained, to reduce communication costs
% between tasks.  Also, we wish to directly support multiple levels of
% hardware parallelism, which indicates that a hierarchical load
% balancing approach may be desired.  Hierarchical load balancing would
% allow for balancing between different parallelization levels at
% different frequencies, e.g.~balancing between nodes less frequently
% than between cores within a node.  It would also allow balancing with
% respect to different metrics, e.g.~balancing memory usage between
% nodes and computation between cores within a node.  Scalability also
% implies that the cost of load balancing should be strictly less than
% $O(P)$ and less than $O(N)$ per process.
% 
% %-----------------------------------------------------------------------
% \subsection{Other requirements} \label{ss:require-other}
% %-----------------------------------------------------------------------
% 
% Below we summarize other requirements, both functional and
% non-functional.
%   
% %-----------------------------------------------------------------------
% \subsubsection{User functions} \label{sss:require-user}
% %-----------------------------------------------------------------------
% 
% While the framework will support adaptive mesh refinement for both
% grid and particle data, algorithms and other problem-dependent
% functions must be provided by the user.  User functions may be either
% Fortran or C (or \cpp).
% 
% The main functions users supply will be numerical algorithms for
% advancing a patch, hierarchy level, or full hierarchy one time step.
% Inline-analysis and visualization functions may also be provided by
% the user.  Patch-based user functions will have access to the
% associated mesh and particle data, plus a bounding layer of ``ghost''
% or ``guard'' zones around the grid block, as well as an analogous
% layer of nearby particles.
% 
% Other user functions will include those for imposing inter-level
% constraints such as flux correction, transferring mesh data between AMR
% levels, and evaluating refinement criteria to locally refine or
% coarsen the mesh.
% 
% %-----------------------------------------------------------------------
% \subsubsection{Run-time parameters} \label{sss:require-parameters}
% %-----------------------------------------------------------------------
% 
% Run-time parameters, such as those defining the domain, initial
% conditions, AMR data structure properties, parallelism approach,
% numerical method parameters, mesh fields and particle groups, etc.,
% will be specified using a single or multiple included parameter files.
% User code will have access to all parameters, including those
% associated with the framework.
% 
% The parameter file grammar and data types should be powerful enough to
% specify user problems; in particular, it should be powerful enough to
% define any existing \enzo\ test problem for the \enzoii\ driving
% application, despite not having problem-specific parameters and
% initialization implemented in the code.  Since initial conditions are
% typically specified by setting fields equal to different numerical
% expressions in different subregions of the domain, the parameter file
% grammar must be able to support both arbitrary scalar expressions for
% field values, and arbitrary logical expressions involving spacial
% variables (e.g.~\code{x}, \code{y}, and \code{z}) for specifying
% subregions of the domain.  Common data types such as scalars, strings,
% and lists or arrays should be supported as well.  Field and particle
% initial conditions may also be defined in terms of user code or data
% files.
% 
% The range of parameters should be both broad and deep, allowing
% flexible definition of simulations and detailed control of data
% structures and methods.  The expected large number of parameters
% indicates a hierarchical organization of parameters, e.g.~explicit
% grouping of parameters based on related functionality to improve
% parameter file readability.
% 
% %-----------------------------------------------------------------------
% \subsubsection{Interfaces} \label{sss:require-interfaces}
% %-----------------------------------------------------------------------
% 
% The framework must be able to communicate with users while running, to
% provide clear output of the state and progress of the simulation.
% This should include an estimate of progress, performance, list of any
% warnings or errors, as well as method-specific information such as
% solver iterations or method-specific warnings or errors.  This
% indicates that user code as well as the \cello\ framework should have
% access to functions for communicating information to users in a
% running simulation.
% 
% The mode of output should not be limited to text, but should include
% support for generating simple plots of data fields and particle
% groups, and graphs indicating the progress or performance of the
% simulation.
% 
% All output should be easily accessible to the user, and include
% varying levels of detail, from high-level summary information to more
% in-depth detailed data.
% %-----------------------------------------------------------------------
% \subsubsection{I/O } \label{sss:require-io}
% %-----------------------------------------------------------------------
% 
% I/O must be efficient, scalable, and portable, indicating a parallel
% I/O library such as HDF5 be used, and a standardized AMR file format
% such as that provided by the ADIOS API~\cite{LoKl08} be adopted.  Data
% dumps should include all or a subset of data fields and particle
% groups, and different sets of data may be written at different
% intervals, times, or when certain user-defined conditions are met,
% depending on post-processing requirements.
% 
% %-----------------------------------------------------------------------
% \subsubsection{Performance}  \label{sss:require-performance}
% %-----------------------------------------------------------------------
% 
% Performance is a non-functional requirement, but crucial to the
% development and behavior of the framework.  We desire high
% performance, high utilization, and high scalability of all available
% computational, memory, communication, and disk hardware components.
% This implies high performance and scalability at all levels of the
% software, including data structures, algorithms, communication, I/O,
% and external libraries.
% 
% We wish to scale a problem of size $N$ efficiently to $> P = 10^6$
% cores, which indicates that memory and computation should be $< O(P)$
% per compute process, as well as $\leq O(N/P)$ per process.  This
% indicates that $O(N)$ data structures must be distributed.  Since MPI
% implementations themselves are not necessarily $< O(P)$ per process
% for large communicators~\cite{BaBu09}, this further motivates using a
% hierarchical parallelism approach to restrict communicator sizes.
% 
% %-----------------------------------------------------------------------
% \subsubsection{Resilience} \label{sss:require-resilience}
% %-----------------------------------------------------------------------
% 
% The importance of fault tolerance and software resilience increases
% with the number of components in an HPC platform.  As platforms
% continue to grow in size, faults will transition from being occasional
% events into being a continuous stream of failures.
% 
% Our framework will attempt to be resilient to multiple compute,
% memory, disk, interconnect, and software failures.  Its success will
% depend on the ability to detect failures, which may require OS or
% library support that is not currently available.  Even if these
% requirements may not be fully realizable yet, resilience still
% indicates certain characteristics of the software framework.  These
% include virtualization of processors and nodes, the ability to flag
% certain hardware or software components as unreliable, the flexibility
% to adapt distributed data structures to the reduced hardware
% availability, and the ability to continue the simulation despite
% multiple failures.
% % also flag components that may soon fail due to overheating, etc.
% 
% The software will also support recovering from numerical errors,
% either by dynamically reducing the time step, dynamically modifying
% method parameters, or switching to a compatible but different
% user-supplied numerical method for solving the same problem.  For
% example, switching from a fast but sensitive linear solver to a slower
% but more robust one.
% 
% The standard checkpoint/restart approach to fault tolerance will be
% supported, though this is unlikely to scale well due to the disparity
% in performance between disk I/O and computation, both in magnitude and
% rate of change.  The software will also support checkpoint to
% memory; this approach scales better, but of course reduces the amount
% of available memory for the application.  As with detection of
% failures, the checkpoint/restart functionality may be outsourced to
% an external library, such as the Berkeley Lab Checkpoint Restart
% (BLCR) library~\cite{wwwblcr}.


%=======================================================================
\section{Software Design} \label{s:design}
%=======================================================================

% \begin{verbatim}
% Design goals
%   extreme parallel scalability
%   extreme data structure scalability
%   breadth: number of grid patches
%   depth: number of AMR hierarchy levels
% Overall approach
%   aggressive minimization of synchronization and collective operations
%   ``localize'' problem as much as possible
% \end{verbatim}

Our design will involve primarily object-oriented programming (OOP), a
proven software design and implementation paradigm that helps control
software complexity and improve code maintainability and reuse.  We
will also investigate aspect-oriented programming (AOP) techniques,
which address cross-cutting software concerns that the OOP model does
not handle effectively.  Below is the current component diagram for
the high-level design of \cello.

\FIGURE{\cello\ software component diagram}{f:components}{
\begin{minipage}{6in}
\begin{center}
\includegraphics[width=3.5in]{components2.pdf}
\end{center}
\end{minipage}}

% separate cross-cutting concerns, such as logging state information,
% memory management, accessing run-time parameters, and detecting and
% resolving hardware or software faults.

High-level design of the framework involves functionally decomposing
the proposed software into a collection of large-scale components or
packages.  Components will be organized into three layers, with
software dependencies directed downwards; cross-cutting software
concerns will also be implemented as components.  Each component will
be implemented as one or more inter-operating classes.  Medium level
design will involve defining interfaces and class hierarchies for each
component, and low-level design will involve defining class attributes
and implementing individual class member functions.

%  Components are partitioned into top-level, middle-level,
%bottom-level, and cross-cutting concerns.  Since the software development
%process is iterative, the final component diagram may still evolve
%somewhat.

% \FIGURETWOCOL{Software component diagram}{f:components}{
% \begin{minipage}{3in}
% \includegraphics[width=3in]{components2.pdf}
% \end{minipage}}


The top-level layer of components includes \code{Task} for encapsulating parallel
tasks, \code{Distribute} for distributing and load balancing tasks
across available compute resources, and \code{Schedule} for
dynamically scheduling the tasks for execution.
%
The middle-level layer of components includes \code{Simulation} for
defining the simulation, and \code{Method} for encapsulating application
functions for performing the actual numerical computation, inline
analysis, and visualization processing required to run the simulation.
Distributed AMR is implemented in the \code{Amr} package, with related
components \code{Field} and \code{Particles} for representing data
fields and particle groups defined on the AMR hierarchy.  \code{Array}
is a component for defining and manipulating distributed Fortran-like
arrays.
%
The bottom-level layer of components includes API's for interacting
with hardware, including \code{Disk} for disk I/O, \code{Memory} for
dynamic memory management, and \code{Parallel} for parallel
synchronization and data transfer.  This bottom-level layer is helpful
for incorporating error checking and performance monitoring, and
encapsulates library calls, such as HDF5 in \code{Disk}, and multiple
parallel technologies such as MPI, UPC, and OpenMP inside
\code{Parallel}.
%
The components associated with cross-cutting software concerns include
\code{Parameters} for reading, storing, and accessing run-time
parameters, \code{Monitor} for logging performance and status
information, \code{Portal} for dynamically interacting with external
utilities or systems, \code{Performance} for collecting and accessing
performance-related data, and \code{Error} for signalling hardware or
software faults.

% Component sizes will vary: \code{Amr} is expected to be large since
% representing and manipulating distributed AMR hierarchies is
% inherently complex, whereas \code{Memory} is likely to be relatively
% small.  Larger components will be further decomposed into smaller
% sub-components, e.g.~\code{Amr} may contain \code{Tree}, \code{Patch},
% \code{Level}, and \code{Box} subcomponents, to help organize and
% control software complexity.  Larger components may be implemented
% using several interacting class hierarchies, whereas smaller
% components may be implemented as a single class.

Below in sections \S\ref{ss:design-amr} through
\S\ref{ss:design-other} we describe key design issues of the \cello\
framework.
%related to requirements listed in sections \S\ref{ss:require-amr}
%through \S\ref{ss:require-other}, respectively.

%-----------------------------------------------------------------------
\subsection{Adaptive mesh refinement} \label{ss:design-amr}
%-----------------------------------------------------------------------

How the AMR approach is designed is crucial for it to meet our
stringent scaling and performance requirements.  It must maintain high
parallel efficiency throughout the range of single-level ``unigrid''
problems, through ``wide'' problems requiring millions of patches, and
``deep'' problems requiring several dozens of levels.  All of these
regimes must be efficiently mapped to HPC architectures with millions
of multi-core processors, and with limited physical memory capacity
per node.

The most important AMR data structure design decision is which AMR
variant to use.  We have carefully weighed the advantages and
disadvantages of the two leading variants, block-structured AMR (SAMR)
and octree-based AMR, and have decided on a modified octree-based AMR
approach.  Our proposed modifications, which we review below, will
significantly improve AMR efficiency for both ``wide'' and ``deep''
AMR simulations, which will in turn enable new and important
multiresolution multiphysics scientific simulations.

% We feel that octree-based approaches are arguably more
% scalable~\cite{BuGh08}, in part because they avoid the parallel patch
% placement algorithms required by SAMR, which can be a significant
% hindrance to scalability~\cite{GuWi06}.  
%Also, high quality SAMR
%frameworks such as \chombo\ and \samrai\ are already available and
%being actively developed.  
% Also, the octree in octree-based AMR can be leveraged for fast
% particle neighbor searches; octrees have particularly efficient
% representations and operations~\cite{FrPe02}; and absolute patch
% extents are computed not stored, which can be used to address some of
% the precision and range issues that arise for extremely large or deep
% AMR hierarchies.

% Standard octree-based approaches involve either an octree or a forest
% of octrees.  Nodes of the octree typically correspond to small
% fixed-size locally-Cartesian grid blocks, such as the $4^3$ grid
% blocks used in the FLASH astrophysics application built on the
% \paramesh\ framework.  Refining a tree node involves subdividing the
% node into eight octants, and coarsening involves the inverse
% operation.  Grid blocks can be associated with either all nodes in the
% tree, or just the leaf nodes.  If the tree supports particle data as
% well, particles are generally associated with leaf nodes.  To
% eliminate ``level jumps''---a jump of more than one refinement level
% between adjacent grid blocks---a ``balancing'' operation is performed
% by refining all coarse blocks that are adjacent to blocks that are
% greater than one level finer.  Efficient methods have been developed
% for this operation~\cite{fast-balance}.  Time steps are globally
% determined, but may be different for different AMR levels.

% Parallelizing involves distributing $N$ tree nodes across $P$
% processors, which is frequently accomplished using a space-filling
% curve.  First blocks are linearizing using a linearized Morton, Gray
% code, or Hilbert ordering, then the list is divided evenly into $P$
% sections, with blocks in the $k$th section assigned to the $k$th
% processor.

There are several operations required of the AMR data structure, all
of which must be designed and implemented to maintain high performance
and scalability.  These include generating the initial grid hierarchy,
dynamically refining and coarsening grid patches or blocks,
maintaining a balanced AMR data structure free of level jumps,
defining and distributing parallel computational tasks, and
maintaining a uniform work distribution by dynamically load balancing
tasks.  The AMR data structure must also support efficient access to
mesh and particle data, and control data locality at all levels to
optimize inter-node communication and use of deep memory hierarchies.
Below we summarize salient features of our AMR approach, and how they
satisfy our rigorous performance and scaling requirements.

%------------------------------------------------------------------------

\SUBSUBSECTION{Patch coalescing}
%
By only supporting small fixed-sized blocks, the number of AMR blocks
can grow very large in regions of uniform resolution in the standard
octree-based approach.  One proposed enhancement is to allow variable
grid sizes per AMR hierarchy block.  This can be introduced using the
simple invertible AMR-invariant operation illustrated in
Figure~\ref{f:coalesce}.  While keeping the underlying mesh resolution
constant, the operation transforms multiple AMR patches into a single
patch, but with a larger grid block.  We call this \textit{patch
  coalescing}.  The inverse operation is permitted as well if the
patch requires subsequent refinement or coarsening.

% \FIGURETWOCOL{$2$D illustration of patch coalescing}{f:coalesce}{
% \begin{minipage}{2.5in}
% \includegraphics[width=2.5in]{coalesce.pdf}
% \end{minipage}}

\FIGURE{$2$D illustration of patch coalescing}{f:coalesce}{
\begin{minipage}{3.75in}
\includegraphics[width=3.75in]{coalesce.pdf}
\end{minipage}}


This operation decouples the AMR topology from the local resolution
requirements, which permits a more efficient AMR data structure for
the same resolution requirement.  Patch coalescing will be
particularly efficient for AMR problems that involve large regions in
which the resolution is fine but uniform.  We note that this
modification is not relevant to patch-based AMR (SAMR), since patch
sizes are already variable.

As a proof-of-concept, in Figure~\ref{f:cosmo} is an image of a 2D
cosmology density field projection, together with two balanced 2D
quadtrees adapted to the image's gray scale.  One of the quadtrees
used fixed-resolution patches, whereas the other used patch
coalescing.  Even though the example problem is not particularly
well-suited for this technique, the resulting quadtree nevertheless
has $2.5$ times fewer nodes.

% \FIGURETWOCOL{Coalesced patches example} {f:cosmo}{
% \begin{minipage}{3.2in}
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{cosmo2-invert.png}
% \end{minipage} \
% \begin{minipage}{1.5in} \raggedright
% \small Coalesced patches using a 2D cosmology
%   density field projection.  \textbf{Top left}: Source density field.
%   \textbf{Bottom left}: Balanced quadtree with 81701 patches.
%   \textbf{Bottom right}: Balanced quadtree with 32529 coalesced patches.
% \end{minipage}
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{cosmo2-4-1-inv.png}
% \end{minipage} \ 
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{cosmo2-4-2-inv.png}
% \end{minipage}
% \end{minipage}
% }

\FIGURE{ Coalesced patches using a 2D cosmology density field
  projection.  \textbf{Left}: Source density field.  \textbf{Middle}:
  Balanced quadtree with 81701 non-coalesced patches.  \textbf{Right}:
  Balanced quadtree with 32529 coalesced patches.  } {f:cosmo}{
\begin{minipage}{\textwidth}
\begin{center}
\begin{minipage}{2.0in}
\includegraphics[width=2.0in]{cosmo2-invert.png}
\end{minipage} \ 
\begin{minipage}{2.0in}
\includegraphics[width=2.0in]{cosmo2-4-1-inv.png}
\end{minipage} \ 
\begin{minipage}{2.0in}
\includegraphics[width=2.0in]{cosmo2-4-2-inv.png}
\end{minipage}
\end{center}
\end{minipage}
}

While allowing flexible patch block sizes can greatly reduce the size
of the octree-based AMR data structure, the variable block size can
complicate other issues, specifically load balancing and parallel task
definition.  Load balancing is complicated since work load per grid
patch is no longer ``constant'' within a level.  We address this issue
by allowing parallelism within a single patch block, which decouples
parallel tasks from AMR blocks.  With this approach, which we describe
in more detail in \S\ref{ss:design-parallel}, we can maintain a
constant or bounded subblock task size despite permitting arbitrarily
large AMR patch blocks.  Decoupled parallel task sizes, together with
patch coalescing, will allow us to regain ``unigrid performance'' in
localized regions of the domain that have uniform but fine resolution
requirements.

%------------------------------------------------------------------------

\SUBSUBSECTION{Targeted refinement}
%
Another limitation of the standard octree-based AMR approach is that
the data structure can grow excessively large for ``deep'' AMR
problems, due to the combination of relatively ``shallow''
refinement-by-two, and the octree balancing steps for removing level
jumps.  To address this, we propose to support $4^3$-trees with
refinement by $r=4$, and even $8^3$-trees with refine by $r=8$, in
addition to $2^3$-trees (octrees) with refinement by $r=2$.  We call
this generalized $r^3$-tree approach \textit{targeted refinement}.

The motivation and main advantage of targeted refinement is for
particularly deep AMR runs, where the region of interest is tiny
relative to the global domain size, such as simulations of galaxy or
star formation.  As a proof-of-concept example, in Figure~\ref{f:dots}
two balanced $r^2$-trees are shown refined on a small set of points in 2D, one
with $r=2$ and one with $r=4$.

% \FIGURETWOCOL{Targeted refinement example}
% {f:dots}{
% \begin{minipage}{3.2in}
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{dots-invert.png}
% \end{minipage} \ 
% \begin{minipage}{1.5in}
% \small Targeted refinement using multiple point sources in 2D.
% \textbf{Top left}: Point sources.  
% \textbf{Bottom left}: Balanced octree with 2137 patches.
% \textbf{Bottom right}: Balanced octree with 158 explicit patches.
% \end{minipage}
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{dots-4-1-inv.png}
% \end{minipage} \ 
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{dots-16-5-inv.png}
% \end{minipage}
% \end{minipage}}

\FIGURE{
Targeted refinement using multiple point sources in 2D.
 \textbf{Left}: Point sources.  
 \textbf{Middle}: Balanced octree with 2137 patches.
 \textbf{Right}: Balanced $4^3$-tree with 158 explicit patches.
}
{f:dots}{
\begin{minipage}{\textwidth}
\begin{center}
\begin{minipage}{2.0in}
\includegraphics[width=2.0in]{dots-invert.png}
\end{minipage} \ 
\begin{minipage}{2.0in}
\includegraphics[width=2.0in]{dots-4-1-inv.png}
\end{minipage} \ 
\begin{minipage}{2.0in}
\includegraphics[width=2.0in]{dots-16-5-inv.png}
\end{minipage}
\end{center}
\end{minipage}}

The advantage is obvious: the number of nodes in the AMR hierarchy is
reduced by a factor of about \textit{13.5}.  Two issues that arise
with targeted refinement---level jumps higher than a factor of $r=2$,
and how to implement the $r^3$-tree data structure---can both be
addressed directly.  First, a resolution jump of at most $r=2$ can be
regained by using an ``implicit backfill'' technique, which involves
inserting a layer of additional patches of intermediate resolution
along level boundaries.  Second, $r^3$-trees can be implemented
directly using octrees by simply skipping over intermediate tree
levels.  For ``deep'' problems, the advantage of steeper refinement
and localized backfill operations will more than offset the additional
cost of the intermediate levels of empty octree nodes.

% There are apparent disadvantages as well.  One disadvantage is that a
% balance $r^3$-tree still has jumps in refinement greater than two.  We
% can regain this mesh constraint by introducing \textit{implicit
%   backfill} patches, as indicated by the magenta patches in
% Figure~\ref{f:backfill}.  We refer to these as ``implicit'' because
% they are not part of the $r^3$-tree data structure, but rather are
% associated with fine-coarse level interfaces.


% \FIGURETWOCOL{Targeted refinement with backfill}{f:backfill}{
% \begin{minipage}{3in}
% \includegraphics[width=3in]{kd-backfill2.pdf}
% \end{minipage}}

%  \FIGURE{Targeted refinement with backfill}{f:backfill}{
%  \begin{minipage}{6.15in}
%   \begin{center}
%  \includegraphics[width=4in]{kd-backfill2.pdf}
%   \end{center}
%  \end{minipage}}

% Another disadvantage is that we need to represent the $r^3$-trees.
% This can be done by simply using the octree data structure
% implementation and skipping over intermediate tree levels.  This still
% reduces the overall data structure size, since the balancing step is
% only performed on the ``active'' levels.  This can also improve
% parallel scaling: the octree balancing operation potentially affects
% all levels of the hierarchy,.whereas for $r>2$ the $r^3$-tree
% balancing and backfill operations are much more localized, as
% indicated by the example in Figure~\ref{f:backfill}.

%------------------------------------------------------------------------

\SUBSUBSECTION{Level windows}
%
For particularly deep AMR runs, the amount of parallelism is
relatively small, since the serial bottleneck is time stepping on the
finest levels.  Memory consumption can be large, however, especially
relative to the amount of parallelism available; this in turn
restricts the scope of solvable problems due to limited physical
memory available per process.  We plan to address this issue by
deallocating unnecessary storage.  Because of smaller time steps taken
on finer levels, the entire simulated time of interest can be less
than a single time step at the coarser levels.  If it is known that
data at a coarse level will no longer be required, then that level can
simply be removed from the simulation, freeing up resources for finer
levels.  In principle, this could allow arbitrarily deep AMR runs,
since only a fixed interval or ``window'' of AMR hierarchy levels
would be required.
%  This would only be feasible
% if other data structure limits and numerical scaling is handled
% appropriately, and provided the total number of fine level time steps
% is not excessive.

%------------------------------------------------------------------------
\SUBSUBSECTION{Implicit global indices}
%
For particularly large and deep AMR runs, the limited range of global
indices and precision of absolute positions can become problematic.
The limits of 32-bit values can quickly be exhausted, and for extreme
AMR even 64-bit values are insufficient~\cite{BrAb01}.  This can lead
to increased memory usage for storing the AMR metadata, and increased
code complexity to support non-standard 128-bit values.  Our approach
will be to support the option of not storing global indices and
positions for the supporting AMR data structure, and only compute them
when absolutely necessary.  The octree data structure defines the
relative configuration of a block to its parent, neighbors, and child
blocks, which is sufficient for most operations---in particular, only
knowing the local mesh width and time step is required for many
hyperbolic conservation problems.  Some operations may of course still
require absolute positions, such as problem initialization or inline
analysis routines.  In these cases, absolute positions or indices can
still be determined using the octree data structure in $O(\log N)$
time using whatever precision or range is necessary.  A similar
approach will be used for particle position data.

%------------------------------------------------------------------------

\SUBSUBSECTION{Patch-local adaptive time steps}
%
There are two common variants in time step determination: uniform time
steps over the entire domain, and adaptive time steps at each
refinement level.  These variants have their relative advantages and
disadvantages: adaptive time steps are more computationally efficient,
especially for ``deep'' AMR runs, whereas using uniform time steps is
easier to implement and parallelize since all patches can advance
simultaneously.  A disadvantage of both is that time step
determination, either for the entire domain or within individual
levels, typically requires a global reduction operation.  However,
global operations are particularly costly for massively parallel
machines, and we believe this global operation can be circumvented by
relaxing the constraint of fixed time steps within AMR levels.  The
time step is determined by stability constraints, which are largely
proportional to the local spacial resolution; however, in highly
dynamic regions such as rapid gravitational collapse, smaller time
steps may be required.  Instead of allowing these isolated regions to
determine the time step for the entire level, we will optionally
permit individual patches to make multiple time steps relative to
other patches within the same level.  This will eliminate the
expensive global reduction operation, but will still maintain
numerical accuracy and satisfy required stability constraints.

%------------------------------------------------------------------------

\SUBSUBSECTION{Octree AMR data structure}
%
Some AMR applications and frameworks store the entire hierarchy
metadata redundantly on each processor, which limits scalability.
Other AMR codes---primarily octree-based ones---distribute the
hierarchy among processors, using a space-filling curve or similar
approach.  Each has its advantages and disadvantages: distributing is
more scalable, but duplicating can be more efficient because of
reduced communication.
% Storing octree data
%structures in particular is extremely efficient, requiring at most
%three pointer variables per node~\cite{FrPe02}.  However, the approach
%still ultimately does not scale, because it requires $O(N)$ storage
%and computation per process, and since available physical memory per
%process is always limited.  As the problem size grows, eventually the
%AMR data structure storage will exhaust all available physical memory.
Our solution will be to support both in a hybrid
duplicated/distributed AMR data structure.  Coarser levels, which span
wider areas of the domain, can be stored redundantly (but still
efficiently, requiring storing at most three pointer variables per
octree node~\cite{FrPe02}) on every process.  Grid patches in finer
levels---along with a layer of adjacent ``remote proxy patches,''
analogous to ghost zones for AMR patches---will be stored only on the
local processes to which the patches are assigned.  The exact
refinement level at which the switch occurs may be arbitrary, may
change during a simulation, and may vary in different regions of the
domain, depending on how the local tradeoff between memory and
communication costs change.

%-----------------------------------------------------------------------
\SUBSUBSECTION{Mesh data}
%
There will be any number of user-defined fields defined on grids
(density, pressure, etc.).  Different fields will be accessed using
opaque field ID's or string identifiers, which will help make the
\cello\ framework independent of the specific scientific problem
domain.  Optional scaling factors may be defined, which may vary for
different fields, different AMR resolution levels, or different
methods; this can be helpful if user's solvers have scale-sensitive
numerical behavior, or if different solvers expect fields to be scaled
differently.  Precision of individual fields may also vary, depending
on the relative requirements for precision and range.
%
Fields will be represented as small locally Cartesian grids associated
with octree nodes or leaves.  Grid block sizes may vary, but parallel
task sizes may be kept constant---this implies that field data
associated with an octree node may distributed across multiple
processes.  In particular, in the limit of a large ``unigrid'' (single
AMR level) run, the AMR data structure will be a single large AMR
patch, and the grid block will be partitioned into at least $P$
blocks.  Ghost zone data may be stored either permanently or on an
``as-needed'' basis.  Ghost zone depths may vary between fields to
improve memory utilization and data movement costs.
%
Users may optionally specify properties of different fields, such as
bounds on individual fields (e.g.~a minimum of $0$ or some small
positive value for pressure), to allow \cello\ to check for software
or numerical errors.   
%Support for
%more general assertions on field values may be implemented as well,
%such as checking that conserved fields are indeed conserved, or
%similar constraints such as $\nabla\cdot B=0$ for MHD problems.
If a property is not satisfied, then some action can be taken to
address the problem, such as reduce the size of the time step
constraint or switch to an alternate solver, thus

%-----------------------------------------------------------------------
\SUBSUBSECTION{Particle data}
%
There will be any number of user-defined particle groups (dark matter,
tracer particles, etc.).  Different particle groups may have different
user-defined attributes associated with each particle in the group.
Particles in different groups will be accessed using opaque particle
group ID's or user-defined string identifiers.
%
The same octree data structure that is used to support
multi-resolution field data will be used to spacially organize
particles.  Particles will be associated with leaf nodes of the
octree, which will enable fast and accurate hybrid particle/mesh
methods such as $PPPM$~\cite{HoEa88}.
%
Particle positions will be optionally represented using local
coordinate systems defined in terms of the AMR octree blocks, rather
than in terms of a single global coordinate system.
%This is
%illustrated in Figure~\ref{f:local-particles}.
This will directly address the scaling issue of positional accuracy
deterioration due to catastrophic cancellation, which only occurs if
global positions are stored.  When global positions are required, for
example for analysis or visualization, they can be computed using the
octree data structure in $O(\log N)$ time, using whatever precision or
range is necessary.

% \FIGURETWOCOL{Global (left) versus patch-local (right) coordinate systems for
%   particle positions}{f:local-particles}{
% \begin{minipage}{3.2in}
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{particles-global.pdf}
% \end{minipage} \ 
% \begin{minipage}{1.5in}
% \includegraphics[width=1.5in]{particles-local.pdf}
% \end{minipage}
% \end{minipage}}

% \FIGURE{Global (left) versus patch-local (right) coordinate systems for
%   particle positions}{f:local-particles}{
% \begin{minipage}{6.8in}
% \begin{minipage}{3.3in}
% \includegraphics[width=3.3in]{particles-global.pdf}
% \end{minipage} \ 
% \begin{minipage}{3.3in}
% \includegraphics[width=3.3in]{particles-local.pdf}
% \end{minipage}
% \end{minipage}}

% A consequence of using patch-local coordinate systems is that
% particles that migrate from one patch to a neighboring patch will
% require a coordinate transformation.  This transformation is trivial
% to implement and compute.  Also, sometimes global particle positions
% are still required, for example for analysis or visualization.  In
% these cases global positions can still be made available to user
% functions through the \code{Particles} API.  Global positions can be
% internally computed by \cello\ to any required precision by traversing
% the AMR octree with $O(\log N)$ cost, even if they are only stored
% using patch-local coordinates.

%-----------------------------------------------------------------------
\subsection{Parallelism} \label{ss:design-parallel}
%-----------------------------------------------------------------------

For a hyperbolic problem defined on an arbitrary AMR hierarchy, any
given patch in the hierarchy may advance one time step if all of its
boundary data is up to date.  Assuming that the advancement of a grid
patch one time step is taken to be a parallel task, this defines a
dependency graph.  \cello\ will use this dependency graph to
dynamically control the parallel execution of the simulation.

\SUBSUBSECTION{Task definition}
%
Tasks are encapsulated in the \code{Task} software component. Since
the AMR approach will include variable patch sizes, data on a single
patch may be distributed among multiple processes.  The basic approach
of defining a dependency graph will still be used, but subblocks will
be used to define a parallel task.  In \cello, a task may be an entire
patch, or a portion of a patch.  Task sizes can be controlled by
restricting subblock sizes within a given range, which may be
constant.

\SUBSUBSECTION{Dynamic task scheduling}
%
Scheduling and execution of tasks will be controlled by the
\code{Schedule} software component.  \cello\ will maintain a priority
queue of runnable tasks on each process.  Such dynamic scheduling is
well suited for heterogeneous sized tasks and non-regular
communication dependencies. Tasks associated with subblocks in finer
AMR levels can be given higher priority since they determine the
critical path through the dependency graph.  Communication will be
similarly scheduled to effectively pre-fetch ghost data required by a
patch so that it is available when the patch is executed.
%
Execution of tasks and communication
of ghost data will be performed using ``computational registers'' and
``flux registers.''  Computational registers are motivated by
``working block'' and ``work'' data structures in \paramesh, and
``flux registers'' are motivated by \code{LevelFluxRegister}s in
\chombo.  This overall approach will allow decoupling of how data is
stored in the AMR data structure from how it is stored when it is
being actively computed or communicated.  This in turn permits
flexibility to 1) optimize how data is stored when not being
processed, e.g.~without ghost zone data to reduce memory use, 2)
optimize how data is stored when it \textit{is} being processed,
either to ease the interface to user code or to rearrange data to
improve performance on deep memory hierarchies, and 3) optimize how
ghost zone data is communicated, e.g.~to group multiple communication
requests between a pair of processes or nodes together into a single
communication call to improve communication performance.

%-----------------------------------------------------------------------
\SUBSUBSECTION{Load balancing}
%
The \code{Distribute} component will be used to distribute and
dynamically load balance tasks across available computational
resources.  Dynamic load balancing is well known to be a crucial
operation for extreme scalability in general, and for extreme AMR in
particular.  Computational load must be evenly distributed to maintain
high overall parallel computational efficiency, dynamically allocated
memory must be well-distributed across compute nodes to avoid
depleting available physical memory, and data locality must be
maintained within compute nodes to maintain high communication
performance over the interconnect.  We will support several load
balancing techniques, including \textit{space-filling curves} and
\textit{hierarchical load balancing} methods.

Space-filling curves, such as Morton, Gray code, or Peano-Hilbert type
curves, can be used to linearize AMR patches or blocks.  This allows
patches to be evenly partitioned among processes, and helps maintain
data locality to control communication.  This approach has been shown
to scale to over $32K$ cores~\cite{BuGh08}, and works well if workload
and memory usage are proportional to each other within tree nodes,
when workload between tree nodes is roughly equal, and when the
evolutionary change in the hierarchy is sufficiently restricted.
However, we do not consider this approach to be sufficient for a
general-purpose extreme AMR framework.  First, because of variability
and uncertainty in work loads between tree nodes, due to e.g.~adaptive
time stepping, variable particle distributions, and numerical methods
involving localized subcycling.
%
% First, adaptive time steps are required for efficient ``deep'' AMR
% problems, but that scales the work load for a given node by a
% non-constant factor of roughly $2^k$ for level $k$.
%
% Second, particle distributions among nodes may not be uniform, affecting both memory and computational loads.
%
% Third, array patches may vary in size, if we use our ``coalesced
% patches'' enhancement to reduce the AMR tree node count.
% %
% And fourth, performance of physics methods on a patch is not
% necessarily uniform, e.g.~due to localized subcycling of stiff
% methods, extra computations along shock fronts for front tracking
% methods, etc.  
%The first three issues, and possibly the fourth, could
%be addressed by dynamically weighting nodes before partitioning them
%among processes, but that would require additional global
%communication to perform the reduction.
%
Second, this approach will eventually be non-scalable since it
requires $O(P)$ storage per process, and a global all-gather
operation (e.g.~\code{MPI\_Allgather}) is required on all processes.

% Since we suspect that a space-filling curve approach is only effective
% for a strict subset of AMR problems we wish to support, we will also
% explore two ideas for dynamic load balancing, including ``hierarchical
% diffusion-based load balancing'', and an experimental technique we
% call ``deliberate overcompensation''.

%
We will also implement hierarchical load balancing schemes, similar to
that developed in~\cite{LaTa06} specifically for AMR applications.
Hierarchical load balancing has several advantages over more
traditional non-hierarchical load balancing methods, including 1) the
ability to balance with respect to different metrics at different
levels, and 2) balancing at different frequencies between levels.
Balancing with respect to different metrics is particularly useful for
AMR problems, since memory is the crucial metric to keep balanced at
the node level, whereas computational cost should be balanced at lower
hardware platform levels to maintain high processor / core
utilization.  And balancing with different frequencies at different
levels will improve overall load balancing costs, since balancing at
higher, more costly levels can be performed less frequently.  We also
wish to explore hierarchical diffusion-based approaches~\cite{ScKa97}.
Such approaches could be more scalable than space-filling curve
methods, yet, because of the hierarchical nature, still maintain a
globally as well as locally balanced workload.

%as well as locally 
%(This is analogous to multigrid iterative linear system
%solvers, which apply local error reducing relaxation operators at a
%hierarchy of resolution levels to reduce the error globally.)
% Load balancing
% schemes can be either global or local.  Global schemes, such as
% space-filling curves or spectral methods, are considered to either
% generate lower-quality task partitionings quickly, or produce
% high-quality partitionings slowly~\cite{ScKa97}.  Local schemes,
% such as diffusion-based methods, are considered to be efficient
% at balancing tasks locally, but cannot effectively balance tasks
% globally.  Due to the hierarchical nature of HPC platforms, we believe that a
% hierarchical load balancing scheme will be the most effective approach
% for both scalability and efficiency.  By \textit{hierarchical load
%   balancing} we mean balancing tasks between higher levels (nodes or
% supernodes) independently from balancing between lower levels
% (socket or cores).  The idea of hierarchical task scheduling and load
% balancing has been explored for at least 15 years~\cite{AhGh94}, and
% is being actively adapted specifically to AMR
% applications~\cite{LaTa06}, but is not commonly available in currently
% existing AMR frameworks.
% 

% Additionally, we wish to  pursue a hierarchical diffusion-based
% approach~\cite{ScKa97}.  We expect that such an approach could be
% efficient at balancing tasks globally as well as locally, due to
% hierarchically balancing across both low-level and high-level hardware
% components.  (This is analogous to multigrid iterative linear system
% solvers, which apply local error reducing relaxation operators at a
% hierarchy of resolution levels to reduce the error globally.)  Such a
% hierarchical diffusion method could also be implemented with strictly
% less than $O(P)$ cost, since only a fixed amount of localized task
% information is required for each hierarchical level.

% \textbf{Deliberate overcompensation technique.}
%
% Another new technique we wish to explore is \textit{deliberate
%   overcompensation}.  By deliberate overcompensation we mean balancing
% by relocating \textit{more} than enough tasks from high-load to
% low-load processes.  The motivation is that imbalances from physics
% phenomena such as gravitational collapse tend to require a continuous
% and regular redistribution of computational resources.  The deliberate
% overcompensation idea is conceptually analogous to the successive
% over-relaxation (SOR) variant of the Gauss-Seidel method.  Given the
% same tolerance on load imbalance, we expect this approach to improve
% the performance of load balancing by up to a factor of two.
% 
Finally, while load balancing is crucial, it is also still an
unresolved research issue, so we will additionally support
user-implemented load balancing schemes.


% \begin{verbatim}
%  load balancing
%     hierarchical
%        improved scalability
%        improved flexibility
%        improved efficiency
%     distribution approaches
%        replicated
%        distributed
%        hybrid
%          optimize computational performance versus memory usage
%      
%     load balance locally in given level
%     different metrics at different levels
%       memory at node level
%       workload below
%     different frequency at different levels
%       lower frequency balancing at higher levels
%          more costly
%          solution changes less frequently
%       higher frequency balancing at lower levels
%          less costly
%          solution changes more frequently
%     task adjacency maintained
%     use collected performance data
%     explore overcompensation technique
%        ala SOR
%        two regimes
%           local collapsing / explosion
%           shock advancement
%        identify regimes and adapt
%     linearized Morton, Gray code, Hilbert curve insufficient
%        assumes equal work per patch
%        particles are associated with nodes, changing weight
%        adaptive time steps drastically weights more highly  refined patches
%        performance of physics algorithms on a patch is not necessarily uniform
%           localized chemistry subcycling, front tracking, etc.
%        arrays on patches may be different sized (assuming coalesced patches)
%        linearization constricts dimensionality
%           constrains data movement along a single dimension
%           physics imbalances 4 dimensional
%       Morton ordering not always feasible
%          different particle counts
%          adaptive time steps on finer levels
%          physics method variations
%             shock capturing--Riemann solver iterations
%             variable subcycling of iterative stiff methods
% \end{verbatim}
% 
% 
% \begin{verbatim}
% task migration
%    migrate tasks dynamically
%    Charm++ model: pack, relocate, unpack data
%    maintain locality by moving task to owner of a neighbor
%    maintain parent-child locality when possible
%       relax for deep hierarchies
%    migrate using hierarchical parallelism
% \end{verbatim}
% \begin{verbatim}
% Load balance using ``over-compensation'', since heavily-loaded
%   processes tend to continue to become more heavily loaded (cosmology
%   / star-formation application-dependent).
% \end{verbatim}

% (e.g.~load balancing
% across $\approx 2^20$ cores can be replaced by four hierarchical
% load balancing steps across $\approx 2^5$)

% \begin{verbatim}
% OOP
%   addresses 
%     agility, readability, flexibility, modifiability, extensibility, complexity
%   improves component reuse, controls software
%   complexity, eases software maintenance
%   still not always ideal
%      cross-cutting components
%      can use aspect-oriented ideas
%   design patterns
% \end{verbatim}
% 

%-----------------------------------------------------------------------
\SUBSUBSECTION{Parallelization approach}
%
This overall task approach maps well to the data-driven process
virtualization model used by \charm.
% ,
% the functionality of which subsumes that of both the \code{Task}
% (analogous to a ``chare'' object in \charm) and \code{Schedule}
% components, as well as \code{Distribute}, which is used for task
% distribution and dynamic load balancing.  
This similarity is no coincidence, since the \charm\ model helped
influence our high-level parallelization design.  \charm\ also
provides fault tolerance support through its built-in disk, or
memory+disk, checkpoint/restart mechanism.  Because of this close
match in design, we will begin parallel development using the
high-level \charm\ framework.  However, later development will also
include supporting message passing via the MPI library~\cite{wwwmpi},
partitioned global address space (PGAS) programming via the UPC
language~\cite{wwwupc}~\cite{upc}, and shared memory parallel
programming via OpenMP API~\cite{wwwopenmp}.  We will also allow
direct support of hybrid parallelism approaches, including MPI with
OpenMP, and UPC with MPI.

The low-level \code{Parallel} component will encapsulate the core
parallel synchronization and data transfer operations, which will
remove dependencies on MPI, UPC, or OpenMP from other components.  We
expect the \code{Parallel} component will include two parallelization
API's, one for distributed memory and one for shared memory; the
distributed memory API would encapsulate MPI and UPC, whereas the
shared memory API would encapsulate OpenMP and UPC.  The
\code{Parallel} component will also directly support hierarchical
parallelism, which we will use to improve the mapping of data and
communication onto the specific hardware, for example hierarchical
load balancing summarized in the next section.  Additionally, while
heterogeneous platforms such as those containing general purpose
graphics processing units (GPGPU's) will not be directly supported,
the framework's design will facilitate their use by user-written
physics kernels written for GPGPU's.

There are several reasons why we plan to include multiple
parallelization technologies.  First, because the performance and
scalability of the technologies vary between machines and
implementations~\cite{MaTa09}, and we want to always use the fastest
and most scalable approach.  Second, one technology may perform better
in the distributed memory regime, whereas another may perform better
within a shared memory node, which motivates flexible hybrid parallel
approaches.  Third, by including multiple existing parallelization
paradigms from the start, we expect that the resulting software design
will be more amenable to incorporating new parallelization
technologies that may be developed in the future, improving software
sustainability.  And fourth, the framework could be used as a
benchmark tool for parallel technology designers to evaluate their
approaches \cite{WeSu07}.


% While the higher level \code{Schedule}, \code{Task}, and
% \code{Distribute} components will be independent of whether MPI, UPC,
% or OpenMP is used, it will still be dependent on whether \charm\ is
% used.  We still expect to reuse much of the \charm-related code in the
% higher level components when augmenting the code with other
% parallelism technologies, such as functions for ``packing'' and
% ``unpacking'' task-related data required for task migration.  This
% code reuse will help reduce development time.

%-----------------------------------------------------------------------
\subsection{Other components} \label{ss:design-other}
%-----------------------------------------------------------------------

Below we summarize other proposed design solutions for both functional
and non-functional requirements.

%-----------------------------------------------------------------------
\SUBSUBSECTION{Application functions}
%
Applications developed using \cello, including \enzoii, will include
application functions for initializing fields and particles, functions
for advancing the simulation one time step on a single AMR block, and
functions for updating data between neighboring AMR blocks in
different levels.  Other application functions include tagging AMR
patches for refinement or coarsening, and determining the local time
step.  All application functions will have access to certain \cello\
API's, including those for signalling a software or method failure,
defining additional values for user monitoring during a running
simulation, and for accessing application run-time parameters.

% User initialization functions will be required to define the fields
% are particle groups that will be used.  These functions may initialize
% fields and particles as well, or simple initialization can be
% performed using \cello\ run-time parameter files.  Physics functions
% must also be ``registered,'' which may involve declaring which field
% or particle groups are read or modified by each function, whether each
% function is patch, level, or hierarchy-oriented, whether a method
% requires either temporary or persistent local fields or particle
% groups, and how functions are to be sequenced.  User initialization
% functions also assign string identifiers to each method, which are
% used e.g.~to support user method-specific run-time parameters.  To
% improve software resilience, alternate methods may also be specified,
% which \cello\ will invoke in the event the primary method signals a
% failure.

% The primary physics-related user functions will be those that advance
% the field or particle data one time step on a single AMR block.
% \cello\ will schedule and execute these user functions on individual
% patch blocks as outlined in \S\ref{ss:design-parallel}.

% The secondary physics-related user functions will be for computations
% at level interfaces, such as flux-correction or other constraint
% enforcement at AMR level boundaries.  In this case user code will
% operate on fields and particle data defined on two adjacent patches in
% different levels, rather than a single patch.  Scheduling and
% executing these functions will also be controlled by \cello's scheduler.

%-----------------------------------------------------------------------
\SUBSUBSECTION{Run-time parameters}
%
The run-time parameters API will allow access to parameter values by
type, as well as evaluation of scalar and logical expressions.
User-defined parameters can be accessed by user code, so that
parameters specific to the user code can be integrated seamlessly into
the same run-time parameters file as the \cello\ parameters.  Related
parameters will be grouped into individual sections, e.g.~fields,
particles, AMR, application functions, etc., to improve organization
of the parameter files.  Parameter types will include integers,
scalars, logicals, strings, and lists.  Logical and scalar expressions
involving spacial variables \code{x}, \code{y}, and \code{z}, and time
variable \code{t}, will also be supported to enable defining simple
initial conditions and time-dependent boundary conditions via the
run-time parameter file.  More complicated application domain specific
initial conditions will be supported through optional application
functions.

% %-----------------------------------------------------------------------
% \SUBSUBSECTION{Interfaces}
% %
% User monitoring functionality will be implemented in the
% \code{Monitor} component.  Its API will be callable by other
% components, including user functions, for declaring data to output,
% and the format (text, HTML, simple plots).  The component will have
% access to other components, including \code{Performance} and
% \code{Error}, to allow reporting to the user information regarding the
% simulation's computational, communication, and memory performance, as
% well as any errors or warnings encountered.  Since \code{Monitor} is a
% cross-cutting software concern, an aspect-oriented approach may be
% more appropriate than an object-oriented approach, so we will
% investigate languages for aspect-oriented programming, such as
% \code{Aspect\cpp}~\cite{wwwaspectcpp}
% , though we expect the
%cross-cutting concerns in \cello\ to be minor enough that we can still
%easily work within the limitations of \cpp.

%-----------------------------------------------------------------------
\SUBSUBSECTION{Disk I/O}
%
We expect to implement the \code{Disk} component of \cello\ using
parallel HDF5, following a standardized AMR file format such as the
ADIOS API~\cite{LoKl08}.  I/O is known to be a significant performance
and scaling issue.  Even when it is implemented efficiently, it can
still adversely affect overall performance due to the relatively slow
performance of disk hardware.  We plan to make use of available
compression capabilities of HDF5 to reduce disk traffic and storage.
To improve software resilience, we will include support for error
checking on all reads and writes, allowing the \code{Error} component
to detect faulty disk hardware, mark it as unusable, and continue.  We
will support task parallelism for I/O, using a dedicated subset of
available hardware for disk operations.  This will permit taking
advantage of dedicated I/O nodes that are directly attached to disk
hardware, and allow the rest of the simulation to proceed
concurrently.
%  Disadvantages of this approach will include reduced
%parallelism available for computation, and increased inter-node
%communication, but the overall performance increase may still make it
%a desirable option.

% {\tiny
% \begin{verbatim}
% I/O different output formats for different uses
%    checkpointing
%    analysis
%    visualization
%    general ``data dump''
%    cheaper to rerun and regenerate data to process than dump and reread later
%       inline analysis capability to reduce overall output required
%    checkpoints
%        node / processor independent: software resiliency
%        checkpoints restartable on different configurations / platforms
%    ``accesser code'' included with all output data
% \end{verbatim}
% \begin{verbatim}
% I/O
%    parallel HDF5
%    compression
%    CRC error-checking and retry
%    subset of nodes do I/O
%    detection of faulty disk and mark as unusable
%    different formats for different uses
% \end{verbatim}
% 
% \begin{verbatim}
% Enforce strict control over data storage formats (e.g. files)
%   (see W0009)
% Require that all stored data be accessed through standard
%   interface functions that are independent of specific file formats
%   (i.e., stored datasets are conceptually treated as objects)
% \end{verbatim}
% }

%-----------------------------------------------------------------------
\SUBSUBSECTION{Performance measurement}
%
The \code{Performance} component will provide an API that will enable
measuring hardware, software, and data structure-related performance.
The design of the \code{Performance} component will be motivated by
that of \code{lcaperf}~\cite{wwwlcaperf}, but will include several
improvements to its run-time efficiency.
%Although performance is a non-functional requirement, our
%\code{Performance} component will help us to meet our performance and
%scaling requirements.
%  One simple improvement will be to use opaque handles
% for code regions instead of string constants to reduce overhead.  A
% second improvement will be to support declaring a user attribute as
% ``monotonic;'' for example, a ``\code{level}'' attribute defining the
% current active AMR level would not be monotonic, but a
% ``\code{time-step}'' attribute defining the current time step on the
% root level would be.  Defining appropriate attributes to be monotonic
% will allow detecting when performance data groups will no longer be
% updated, so that the data can be flushed to disk and deallocated to
% reduce memory consumption.
The API will allow access to performance data by other components.
This will be used in the \code{Monitor} component to display pertinent
performance information to the user, and by the \code{Distribute}
component for dynamic load balancing using actual collected
performance measurements rather than estimates.  Collected data include
time and memory usage for different code regions, simulation time steps,
AMR levels, and parallel tasks.
%  The
%\code{Performance} component is a cross-cutting software concern, so
%we will investigate designing and implementing it using an
%aspect-oriented programming model.

% {\tiny
% \begin{verbatim}
% integrated performance monitoring
%    summaries at different hardware levels
%    less frequent at lower levels--more data
%    more frequent at upper levels
%    performance data available to other components
%       load balancing based on actual memory usage / cpu time
%       feedback for adaptivity
%       help identify performance and scaling issues early
%       poor-performance resilient
% \end{verbatim}
% 
% \code{Portal} Component
% 
% \begin{verbatim}
% portal: support interfacing with other codes
%    post-processing solvers
%    data analysis pipeline
%    visualization pipeline
%       use existing library, e.g. Visit
%       Method can include visualization or inline analysis
% \end{verbatim}
% 
% \code{Monitor} Component
% 
% \begin{verbatim}
% monitor: support for interfacing with user while running
%    ``dashboard'' for real-time monitoring state of simulation
% \end{verbatim}


% \begin{verbatim}
% hybrid parallel
%    MPI + OMP
%    MPI + UPC
%    UPC + OMP (?)
%    Charm++ + OMP (?)
%    flexible subset of cores, sockets, nodes, supernodes
%    Task scheduling Charm++ model, but implemented in MPI, UPC, OMP
%    processor-task affinity
% \end{verbatim}

% \begin{verbatim}
% Charm++ 
%   data placement
%   load balancing
%   task scheduling
%   checkpointing for fault tolerance
%   performance monitoring and visualization
% \end{verbatim}

% \begin{verbatim}
% platform hierarchical architecture-aware data structures
%   e.g. MPI communicator for cores in a socket, sockets in a
%     node, nodes in a supernode, supernodes in a machine
%   facilitates hierarchical dynamic load balancing
%     improves dynamic mapping of data structures to hardware components
%     E.g. load balance more frequently at core / socket level to
%       keep functional units busy
%     load balance node / supernode levels less frequently to keep
%       memory usage uniform
%     less frequent because:
%       problem changes less at larger scales
%       balancing is more expensive: larger data sizes, slower
%         interconnects
%     user-defined parameters and metrics for load balancing at
%       different levels
%       dynamically collected performance data can be fed back
%         into hierarchical load balancing algorithm
%   note linearization of octree data structure is insufficient:
%     assumes equal work per patch
%     particles are associated with nodes, changing weight
%     adaptive time steps drastically weights more highly
%       refined patches
%     performance of physics algorithms on a patch is not
%       necessarily uniform, e.g. localized chemistry subcycling, front
%       tracking, etc.
%     arrays on patches may be different sized
%     linearization of patches 
%         reduces flexibility
%         constrains data movement along a single dimension
% \end{verbatim}

% \begin{verbatim}
% Multiple parallelization strategies
%   MPI: + widespread, optimized implementations, familiar
%   MPI: - data replication, difficult to use
%   UPC + easier to use, combines shared memory view with efficient data affinity
%   UPC - no concept of MPI communicator, still under development--not as mature
%   OMP + can be used progressively
%   OMP - not scalable outside of socket / node;  inefficiencies due to false cache sharing
%   GPU + very fast / power efficient when usable
%   GPU - no usable standard, difficult to program, difficult to map problem to hardware
%   Charm++ + higher-level, dynamic scheduling, dynamic load balancing, fault tolerant through checkpointing to other node memory
%   Charm++ - requires learning separate language, separate runtime system, no data prefetching(?)
%     currently not fully realizable for GPU since depends on
%       computational code
%     hierarchical parallelism: MPI + OMP, MPI + UPC, MPI + GPU,
%       etc.
%     advantages of hybrid
%       reduced data replication from MPI distributed memory
%       dynamic parallel threads--use more when helpful, fewer when not
%       UPC
%     disadvantages of hybrid
%       performance hit from data sharing in MPI + OMP
%       MPI and UPC communication cannot (currently) proceed concurrently
%     code for two modes: distributed memory and shared memory
%     parallel tasks: grid patches, arrays, grid patch groups,
%       particle groups
%     flexible data structure parameters (grid patch size, patch
%       decomposition, patch grouping) to dynamically optimize task size
% \end{verbatim}
% 
% \begin{verbatim}
% hierarchical parallelism
%    encourage communication within hardware components
%       sockets within node
%       cores within socket
%       hyperthreads within core
% \end{verbatim}
% 
% \begin{verbatim}
% parallel technology encapsulation / processor virtualization
%   distributed / shared memory
%     MPI (two-sided and one-sided) (distributed memory)
%     OpenMP (shared memory) 
%     UPC (either distributed memory or shared memory)
%     Charm++
%   multiple strategies enhance software resiliency
%     i.e. buggy MPI implementation--dynamically switch to UPC
% \end{verbatim}
% }

%-----------------------------------------------------------------------
\SUBSUBSECTION{Software resilience and self-management.}
% Like performance, resilience is another non-functional requirement.
% Also like performance, despite being a non-functional requirement, a
% software component, \code{Error}, will be implemented to aid in
% meeting the requirements.
The \code{Error} component will be used to flag faults related to
either hardware or software.  Other components will call the
appropriate function in the \code{Error} API to flag an error, such as
\code{Disk} to flag a disk fault, \code{Memory} to flag a memory
allocation error, \code{Parallel} to flag a communication or
synchronization error, or an application function in \code{Method} to
flag an algorithmic or numerical error.  Some errors will be easier to
detect than others, and some may not currently be possible without
further OS or software library support.  The \code{Error} component
will include a list of hardware and software components that are
faulty, and will provide other components access to the list, so that
the faulty components can be isolated, any lost data can be recovered,
and the simulation can proceed despite multiple hardware or software
failures.  The \code{Error} component directly supports the self-management
of \cello, providing support for its self-monitoring, reconfiguration,
and ``healing''.

%.  For example, a list of faulty nodes will be kept, which will
% be accessible by the \code{Parallel} and high-level parallelization components,
% so that they will isolate the component, recover any lost data, and proceed
% with the simulation if possible.

% {\tiny
% \begin{verbatim}
%  FT-MPI 
%   ``fault-tolerant MPI''
%   http://icl.cs.utk.edu/ftmpi/overview/index.html 
% MPICH-V 
%   ``MPI Implementation for Volatile resources''
%   http://mpich-v.lri.fr/index.php 
% \end{verbatim}
% 
% 
% \begin{verbatim}
% software resilience
%    take advantage of only Methods change data
%    methods signal which fields / particles changed
% \end{verbatim}
% \begin{verbatim}
% fault-tolerance / software resilience strategies
%    need to deal with continuous stream of failures
%    MTTF < MTTC
%    checkpoint to disk
%       issue: failures will become more frequent than time to checkpoint
%       aggressively reduce checkpoint data size and write time
%          dedicated I/O nodes
%          compress
%          check data
%          methods identify which data modified
%             may help lower disk output--only checkpoint modified data
%    checkpoint to memory
%      Charm++ does this(?)
%    detect hardware errors and mark as defective
%        memory
%        disk
%        core
%        socket
%        node
%        interconnect (pairs of nodes)
%        software libraries (MPI versus UPC, etc.)
%    flash memory
%    log faults to disk for subsequent analysis
%    performance resilience
%       dynamically adapt to reduce cache thrashing / inefficiency
%           array blocking or padding in computational array registers
%       adapt AMR patch size, refinement factor (2,4,8)
%    fault-tolerant MPI
%       FT-MPI
%    leverage new approaches when available
%       active research area
%       keep up to date in latest practices
%       design software to use new approaches
% \end{verbatim}
% }

%=======================================================================
\section{Software Implementation} \label{s:implementation}
%=======================================================================

\cello\ will be written primarily in \cpp, with C and Fortran callable
interfaces to the public API's, allowing user physics functions to be
written in C or Fortran.  It will be written using an object-oriented
approach, with use of design patterns~(\cite{GaHe95} \cite{BuHe07})
where appropriate.  Aspect-oriented programming using
\code{Aspect\cpp}~\cite{wwwaspectcpp} will be investigated for
cross-cutting software concerns.  Additional software libraries used
will include HDF5~\cite{hdf5} for parallel disk I/O, and, in \enzoii's
cosmology initial conditions generation, the SPRNG~\cite{wwwsprng}
library for scalable parallel pseudo-random number generation.

For parallelism, the \charm\ parallelization framework, the MPI
parallel library, the UPC parallel language, and the OpenMP threading
API will all be supported.  Each is optional, so building and running
\cello\ applications will not depend on any single parallelization
technology.  We will attempt to isolate dependencies on \charm\ to the
high-level parallelization components, and isolate dependencies on the
remaining parallelization technologies to the low-level
\code{Parallel} component.

Our development environment involves several components anchored at a
Trac site~\cite{wwwtrac}.
%(\url{http://client65-88.sdsc.edu/projects/cello}, login \code{guest}
%password \code{guest}).
Trac's wiki support is used for brainstorming, organizing, and
refining design issues, after which they are be migrated to formal
\LaTeX\ development documentation.  Trac's roadmap support will be
used for organizing and tracking long-term progress, its ticket
support will be used for individual tasks and bug tracking, and its
browser used for online viewing the subversion source code revision
control repository.  GNU \code{make} is used for controlling
compilation, which we may later migrate to \code{SCons}.  \LaTeX\ is
used for development and user documentation, and Doxygen is used for
directly documenting the source code.


%=======================================================================
\section{Software Testing} \label{s:testing}
%=======================================================================


% Defects can be costly, but they are much more costly when found later
% in the development cycle.  Defects can occur in any phase of the
% development cycle, but those introduced in earlier phases, such as
% requirements and design, tend to be much more costly than those
% introduced in later phases, such as implementation and testing.  To
% improve quality as well as lower development costs, we will emphasize
% our quality control efforts on earlier phases.

Our testing approach will include unit testing, integration testing,
system testing, regression testing, and beta-testing.  Our unit tests
will test individual pieces of code as they are written.  Currently we
use a simple framework of our own design; we may transition to a more
flexible and powerful system such as CppUnit~\cite{wwwcppunit} later,
but our current simple approach has proven satisfactory so far.  Our
integration testing will involve testing the interfaces between
components and subcomponents.  Our system testing will include the
entire \cello\ framework driven by \enzoii\ application functions, and
will include test problems corresponding to those in the existing
\enzo\ test suite.  Our regression testing will include automated
testing of the entire code base using the \lcatest~\cite{wwwlcatest}
parallel software testing environment developed at the LCA.
Regression tests will include tests that expose previously found bugs,
and tests for each of the specific requirements in our Software
Requirements Specifications document.  Beta-testing will involve
existing \enzo\ users, including both those within the LCA, and those in the
wider \enzo\ development community.  The scope of our tests will
include not just correctness, but also performance, scaling, and
resilience at all testing levels.  Performance testing will be aided
by the \code{Performance} component of \cello.

%which will be based on a refinement of the \code{lcaperf}
%package~\cite{wwwlcaperf}, which integrates hardware performance
%metrics with software data structure attributes.

% \begin{verbatim}
%    Improving quality reduces development costs
%       defects costly, especially later in development cycle
%       time spent in finding and preventing defects well worth it
%          more time spent testing and reviewing can paradoxically
%          reduce overall development time
%    Testing + design and code reviews (collaborative construction)
%       Testing complement reviews [CITE code complete]
%       may try pair programming
%    Refactoring to reduce code complexity
%       continually refactor
%       rigorous testing
%    Testing approach
%       unit test all code
%       regression testing
%           use lcatest parallel testing framework
%           correctness, performance, scaling, resilience
%           correctness
%              \enzoii\  implementation
%              compare against Enzo I results
%              existing test problems
%           performance
%              built-in performance monitoring
%              single-thread
%              weak and strong parallel scaling 
%              communication performance
%              I/O performance
%           scaling
%              extreme broad problems (grid / particle counts)
%                 cosmology
%              extreme depth problems
%                 star formation
%           software resilience
%              memory, compute, network, disk, algorithms
%              memory failures
%                 fill
%                 load balancing for memory
%              compute failures
%                 tag component (cabinet, node, socket, core) as unusable
%              network failures
%                 checksums
%                 reroute P1->P2 as P1->Pj->P2
%              disk failures
%                 checksums
%              algorithm failure
%                 support adaptive algorithms
%                 locally override spacial mesh width / time steps
%    Software reviews
%       line-by-line checking
%          by another person, or after time elapsed
%    Refactoring
% development
%    implement progressively to fill user beta testing pipeline
% \end{verbatim}

% \begin{verbatim}
% testing
%    lcatest: automated parallel application testing
%    multiple test levels
%       unit tests
%       component tests
%       application tests
%       in-house / community beta-testing
%          (progressive as functionality comes online)
%    test for multiple things:
%       functionality
%       correctness
%       performance
%       scaling
%    tests also help supplement user documentation
%    use integrated performance monitoring
%       PAPI for hardware counters
%       PMPI for MPI communication
%       new[] / delete[] overload for dynamic memory usage
%          particularly important for AMR
%       user-defined independent attributes
%          cycle
%          level
%          process
%       user-defined dependent metrics
%          process-local patch counts
%          process-local cell / zone counts
%          process-local particle counts
%       less frequent output at finer levels (more data)
%    helps identify functional, performance, scaling bugs early
%    extreme scaling designed into framework from the start
% \end{verbatim}

%=======================================================================
\section{Development Plan} \label{s:plan}
%=======================================================================


% \begin{verbatim}
% application driven
%    helpful for ensuring completeness
%       missing functionality in design will become apparent
%    helpful for testing
%    \enzoii\ 
%    cosmology / astrophysics
%    requires wide range of AMR capabilities
%      broad for galaxy structure formation
%      deep for star formation
%      turbulence
%    requires wide range of physics capabilities
%      hyperbolic: hydrodynamics
%      elliptic: self-gravity, FLD radiation
%      local physics: chemistry, heating/cooling
%    decouple physics modules from AMR framework
%    make both \enzoii\  and underlying AMR framework publicly available
% \end{verbatim}


Our development approach will be to progressively implement \cello\ in
phases, with \enzoii\ as its driving application.  Functionality will
be gradually added, with all phases of the software life cycle updated
with each iteration.  We expect the initial cycle to require the most
effort, since many components and interfaces must be designed before
even the first simple single-level hyperbolic end-to-end simulation
can proceed.

Our basic software development life cycle is based on the standard
requirements, design, implementation, and testing phases.  We iterate
over the phases, progressively adding functionality.  This will help
us ensure that code is correct and scalable at each iteration, and it
allows us to concentrate on small sections of the code at a time
through its entire development cycle.  

Initially individual components will be developed in isolation, then
gradually integrated with each other, strictly controlling
interdependencies.  Prototyping will be used to aid the design
process, to quickly assess the feasibility of new ideas.  Software
components (Figure~\ref{f:components}) and subcomponents are mapped
onto directories and subdirectories in the source code, and class
hierarchies to \code{cpp} and \code{hpp} files.

%While the prototype code will generally
%not end up in the final code, the process of writing it can
%significantly help with writing the final implementation (somewhat
%analogous to the predictor in a ``predictor-corrector'' numerical
%algorithm).

Quality control will be a major ingredient in our development
approach.  High quality software of any type is difficult to develop,
in part because it involves optimizing a litany of desirable
characteristics: parallel software should be maintainable, flexible,
portable, usable, reusable, readable, testable, understandable,
efficient, scalable, reliable, adaptable, accurate, robust, correct,
etc.  We recognize that no single method of maintaining high software
quality is sufficient, so we will employ a suite of techniques that
have been proven to be successful in practice, including testing, code
reviews, design reviews, and refactoring~\cite{Mc04}.

\SUBSUBSECTION{Sustainability} The Laboratory for Computational
Astrophysics has a proven long-term commitment to serving the
scientific community by developing and maintaining world-class open
source scientific software.
%
The predicessor of \enzoii, \enzo, was developed by the LCA in 1994,
has survived 15 years and a factor of 1000 increase in processor
count, and is still a viable parallel scientific code used by hundreds
of scientists worldwide.
%
The LCA plans to continue its development and user support for both
the \enzoii\ application and \cello\ extreme AMR framework well beyond
the scope of this proposal well beyond the lifetime of this initial
development effort.

%=======================================================================
\section{Results from prior NSF support} \label{s:prior-support}
%=======================================================================

[@@@ is prior NSF support needed?]


%=======================================================================
\section{Milestones and Deliverables} \label{s:milestones}
%=======================================================================

%In Figure~\ref{t:phases} we list the major phases of development.
Fully functional releases of the \cello\ AMR framework, and a
corresponding version of the \enzoii\ astrophysics and cosmology
application built on top of the \cello\ framework, will be made
according to the versions listed below.

% \begin{tabular}{|lrl|} \hline
% \multicolumn{3}{|l|}{\textbf{1. Initial Hyperbolic Phase}} \\ \hline
%  &  Physics: & hyperbolic problems\\
%  &  Data: & single level mesh \\
%  &  Parallel: & \charm \\
%  &  \enzoii: & PPM, PPML \\ \hline
% \multicolumn{3}{|l|}{\textbf{2. Basic AMR Phase}}  \\ \hline
%  &  Data: & AMR mesh \\
%  &  \enzoii: & metals, cooling, \ldots \\ \hline
% \multicolumn{3}{|l|}{\textbf{3. Elliptic Phase}}  \\ \hline
%  &  Physics: & elliptic problems \\
%  &  Data: & AMR particles \\
%  &  \enzoii: & PPPM  \\ \hline
% \multicolumn{3}{|l|}{\textbf{4. Advanced AMR Phase}} \\ \hline
%  &  Data: & patch coalescing \\
%  &  Data: & targeted refinement \\ \hline
% \multicolumn{3}{|l|}{\textbf{5. Multi-Parallel Phase}} \\ \hline
%  &  Parallel: & MPI-2, OpenMP, UPC \\
%  &  Parallel: & load balancing \\
%  &  Parallel: & task scheduling \\
%  &  Parallel: & task-parallelism \\ \hline
% \end{tabular}
   

\textbf{6 Months: Version 0.1: Initial Hyperbolic Version.}~The
initial release will support hyperbolic problems on a single-level
``unigrid'' mesh, parallelized with \charm.  This capability is on the
surface simple, but \charm\ is known to have a steep learning curve.
%and the software will be designed to later support a distributed AMR
%data structure, dynamic task scheduling, and dynamic load balancing.
The associated \enzoii\ physics implementation will include both PPM
hydrodynamic (HD) and PPML ideal magnetohydrodynamic
(MHD)~\cite{UsPo09} methods.

\textbf{9 Months: Version 0.2: Basic AMR Version.}~The second release will involve
implementing a basic distributed octree AMR method with level-adaptive
time stepping.  \enzoii\ will be updated to include flux correction
step for PPM and PPML solvers, and other local physics methods will be
migrated from \enzo\ to \enzoii.

\textbf{12 Months: Version 0.3: Elliptic Version.}~This release will involve
implementing distributed particles, as well as support for user level-
and hierarchy-based operations.  \enzoii\ will be augmented with a
PPPM method, including associated level- and hierarchy-based multigrid
linear solvers for solving Poisson's equation for self-gravity.

\textbf{15 Months: Version 0.4: Advanced AMR Version.}~The next release will
involve augmenting the AMR capabilities, including the ``patch
coalescing'' AMR modification to improve ``shallow'' AMR refinement
efficiency, and the ``targeted refinement with backfill'' modification
to improve ``deep'' AMR refinement efficiency.

\textbf{18 Months: Version 0.5: Multi-Parallel Version.}~The fifth
release will involve adding other parallelization support, including
MPI-2 one-sided communication, OpenMP threading, and UPC.  This will
also involve explicitly implementing the task scheduling and load
balancing functionality supported by the higher-level \charm\
framework.

Deliverables for each revision above will include updates of the
\cello\ framework software, \enzoii\ application software, and
comprehensive user documentation, including a user guide, tutorials,
and example problems.  These products will be made publicly available
on the Laboratory for Computational Astrophysics website at
\url{http://lca.ucsd.edu/} for use by the scientific community.
Communication with the user community will be through an email
listserv, and through the \cello\ Trac site.  

\SUBSUBSECTION{Metrics of success} The ultimate success of \cello\
will be defined in terms of the driving application \enzoii, and its
ability to run scientifically viable simulations on the largest NSF
HPC platforms available.  Specific metrics used to quantify success
will include correctness (as verified by comparing \enzoii\ results
with those from \enzo), single-core/processor/node performance,
communication network utilization, disk performance, weak and strong
scalability, and resilience-related metrics to be determined
(e.g.~maximum rate and maximum number of failures the application can
handle).  The primary platforms used for performance and scalability
testing will be \code{Kraken} and \code{Ranger} in the near term, and
\code{Blue Waters} when it comes online.

\SUBSUBSECTION{Software license} Both \cello\ and \enzoii\ will be
distributed under the the \textit{New BSD License}, which has been
vetted as an Open source license by the Open Source Initiative (OSI).



%=======================================================================
\bibliography{papers}
%\bibliographystyle{unsrt}
%=======================================================================

\end{document}

%==================================================================

