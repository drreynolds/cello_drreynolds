([NotesComponentParallelUpc Parallel:Upc] < [NotesComponentParallel Parallel] > [NotesComponentParallelMpi Parallel:Mpi])

[[TOC]]

= Parallel Task Subcomponent =

The `Task` component is used for representing parallel tasks.   It is accessed by both the `Parallel` component for data distribution and dynamic load balancing of data (`Fields` and `Particles`), and the `Control` component for scheduling associated `Method`s.  `Task`s are linked together if they are nearby and expect to share data, e.g. for neighboring patches in an AMR hierarchy.  If linked `Task`s exist on different processes, "ghost" tasks are created and used as proxies for communicating data between the local and remote `Task`s, analagous to ghost zones and active zones in the AMR hierarchy.  

[[Image(htdocs:task.png)]]

== Issues ==

Since `Task`s are controlled by both `Parallel` (for data distribution and load balancing) and `Control` (for scheduling).  The `Task` class must store and provide information required by both `Parallel` and `Control`.  Issues include the following:

 * How are `Task`s initially created?
 * How are new `Task`s created during AMR remeshing?
 * How do `Task`s locate their neighbors?
 * How does a `Task` signal `Control` that it is ready to compute?
 * How does `Control` schedule `Task` execution?
 * How does `Control` schedule inter-`Task` communication?
 * How do `Task`s get transferred to a new process?

== Ghost zone communication ==

There are different ways of communicating ghost zone data between `Task`s, each with different tradeoffs between speed and, if ghost zones are not permanently allocated, storage.  Some variations on communication patterns (independent of how data are transfered) and issues are listed below:

 * Data transfer granularity
  * transfer all ghosts at once: [most costly in terms of storage]
  * synchronous fixed sized blocks of faces at a time
  * asynchronous block of face registers
  * single patch faces
  * individual faces [most costly in terms of message count]
  * pairs of faces between pairs of processes
 * Data communication ordering
  * in increasing order of task x+y+z of centers: assumes unigrid-like blocked task layout
  * presend message to request other processor's `Task` orderings so know what order to send
 * Communication scheduling
  * receive task data, compute task, send task data, ...
  * prefetch K send/recvs using asynchronous block
  * receive x-faces, x-sweep, receive y-faces, y-sweep, ...
  * prefetch K x-faces, ...

 == Communication approaches ==

The main trade-off in approaches below is between storage and communication efficiency.  

 === Enzo-like: [comm - comp]* ===

Enzo exchanges all data in a grid level at once.  This can even be taken further, e.g. at coarse-level timestep all grids in all levels can advance.

  * + fewer synchronization points
  * + smaller message count / bigger messages
  * + compute in place: no data copies
  * + applicable to non-sweep algorithms
  * - need ghost zone sweeps
  * - more communication amount (edges and corners)
  * - need storage for all ghost zones, which can be 2-3 times active ghost zone counts

 === By sweeps: [comm-[xyz] - comp-[xyz] ]  ===

Exchange all X-faces, then compute only x-sweeps, then exchange all Y-faces, etc.
Use flux registers (allocated in a `Block`) to store communicated ghosts so that copies only need to be made on the task being actively computed.

  * + only need to store 1/3 of face ghosts at a time
  * + don't need to store edge or corner ghosts
  * + don't need to sweep in ghost faces
  * + less total communication amount (no edge or corner zones)
  * +? pipelining may be possible to hide communication latency
  * - copies required between sweeps
  * - 3 times as many communication messages
  * - 3 times as many synchronization points
  * - not applicable to non-directionally split algorithms

 === By wavefront sweeps: [ comm-[xyz] (task i=min-to-max [xyz]) comp-[xyz] (task i=min-to-max [xyz]) ] ===

Exchange X-faces a patch at a time, sorting from X-min to X-max.
Option 1: get neighbor X ranges to send ghosts that they need first
Option 2: process M patches at a time

  * + potentially minimal storage, bounded above by 1/3 face zones
  * + no ghost face sweeps necessary
  * - more messages
  * - extra communication to get neighbor X ranges
  * - need to guard against deadlock
  * - not applicable to non-sweep algorithms

 === Patch-by-patch: [comm-i comp-i] ===

Dynamically schedule patches, alternately requesting data when needed, sending data when requested, and computing.  
Option 1: process M patches at a time

  * + potentially small storage, bounded by M
  * + applicable to non-sweep algorithms
  * + asynchronous: similar to what CHARM++ would do
  * - ghost face sweeps necessary for directionally split
  * - extra communication to request data
  * - need to guard against deadlock


=== Patch-by-patch sweeps: [comm-i-[xyz] comp-i-[xyz] ] ===

Dynamically schedule patch sweeps, alternately requesting data, sending data when requested, and computing.

Option 1: process M sweeps at a time

  * + potentially small storage, bounded by M
  * + asynchronous: similar to what CHARM++ would do
  * + ghost face sweeps not necessary
  * - extra communication to request data
  * - need to guard against deadlock
  * - not applicable to non-sweep algorithms

 == Decomposing communication approaches ==

Entities involved in communication at any one time

 * Processes
   * single process pair
   * processes set, e.g. level communicator
   * all processes
 * Tasks
   * single task pair
   * task set, e.g. linear, planar, etc.
   * all tasks
 * Faces
   * single face
   * pair of faces on an axis
   * all faces
 * Direction
   * send
   * receive
   * send and receive
 * Communication buffers
   * In-place
   * Copied to `Block` then in-place
   * Block of multiple fluxes (allocated when needed)
   * Array of flux registers (permanent storage)
 * Scheduling
   * fixed
   * sorted (e.g. by axis for sweeps)
   * query neighbors once
   * query neighbors periodically

 == `Task` class ==

  === Algorithms ===

[[Image(htdocs:tasks.png)]]

  ==== Relocate ====

    A `Task` is associated with "neighbors" and "ghosts".  Neighbors of `Task` `A` represent all other `Task`s that involve direct communication with `A`, e.g. neighboring patches, parent patch, or child patches.  Neighbors may be "local" or "remote", where remote neighbors include information on how to access the data associated with the neighbor, including MPI process, and functions for retrieving data for ghost zone updates or interpolations.  Functions for retrieving data are the same for local and remote `Task`s to help hide communication details from higher code levels.  Ghosts of `Task` `A` are all the "remote" `Task` A's on other processes.  Each `Task` `A` ghost is associated with a remote neighbor of `A`, and each remote neighbor of `A` is associated with an `A` ghost.
    
    Relocate `A` from from process p to process q.

Process p:
    0. Hold all incoming messages to `A` (suspend A)
    1. Process all outstanding incoming messages to `A`.
    2. Create new remote `Task` `A`
    3. Receive `Task` A info from q
    4. Send `A`'s data to q
    5. Initialize `A`'s neighbor links
    6. Update `A`'s neighbors neighbor links 
    7. Send `A`'s ghost links
    8. Delete old local `A`

Process q:
    2. Create new local `Task` A
    3. Send `Task` A info to p
    4. Receive `A`'s data from p
    5. Initialize `A`'s neighbor links
    6. Update `A`'s neighbors neighbor links
    7. Receive `A`'s ghost links
    8. Delete old remote `A`
    9. Lift hold on incoming messages to `A` (release A)
 
  === Attributes ===

 || `Task::ip_`              || ''Owner of `Task` if it's remote. `Parallel` object?`'' ||
 || `Task::id_`              || ''Unique id identifying the task'' ||
 || `Task::state_`           || ''State of the process: idle, running, blocked, suspended, or terminated'' ||
 || `Task::data_`            || ''Pointer to data (`Particles` and `Fields`) and methods (`Methods`)'' ||
 || `Task::neighbor_list_`   || ''List of local tasks to communicate with'' ||
 || `Task::neighbor_states_` || ''State of links between neighbors: sending, receiving, ready, waiting, etc.||

  === Functions ===

 || `Task::Task()`      || ''Create a new task given `Method`, `Field`, and `Particles` (or given `Patch`) ||
 || `Task::~Task()`     || ''Delete a task'' ||
 || || ||
 || `Task::pack()`      || ''Pack data for sending / writing to disk'' ||
 || `Task::unpack()`    || ''Unpack after receiving / reading from disk'' ||
 || `Task::relocate()`  || ''Send `Task` to another process'' ||
 || `Task::write()`     || ''Write `Task` to disk'' ||
 || `Task::read()`      || ''Read `Task` from disk'' ||
 || || ||
 || `Task::begin_send()` || ''Begin sending data to another `Task`'' ||
 || `Task::end_send()`   || ''Complete sending data to another `Task`'' ||
 || `Task::begin_recv()` || ''Begin receiving data from another `Task`'' ||
 || `Task::end_recv()`   || ''Complete receiving data from another `Task`'' ||
 || || ||
 || `Task::create_link()` || ''Create a neighbor link between two `Tasks`'' ||
 || `Task::delete_link()` || ''Delete a neighbor link between two `Tasks`'' ||


  === Usage ===
  === Parameters ===