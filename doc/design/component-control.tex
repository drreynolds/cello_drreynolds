%=======================================================================
\section{Control Component} \label{s:component-control}
%=======================================================================

Given Physics, Algorithms, and Data structures, specify the top-level
sequencing and properties of the simulation.  For example, ordering of
physics modules, whether to do hierarchical time-stepping, up to what
level, whether to sub-cycle some physics, etc. [Is this a useful
category?]  Also include things like floors and limits(?), and IO
dumps

Global simulation timestepping.

Output types and parameters

\begin{itemize}
\item checkpoint (dump all)
\item output (specific fields)
\item movies (type and rate)
\item analysis (type of analysis, rate)
\item level of output (files for timestep, time, etc.)
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Use Cases}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\subsection{Parameters}
%-----------------------------------------------------------------------

Schedules Tasks to advance the Simulation in time 
\begin{enumerate}
\item Control Supporting Classes
\item Control Functions
\item Patch scheduling
\end{enumerate}

The Control component is the main scheduler in Cello, and orchestrates
the distributed computations to advance the simulation in time, as
well as simulation initialization and finalization.  Currently we are
evaluating the use of CHARM++ in the Control component.

\begin{itemize}
\item execute the simulation
    \begin{itemize}
    \item create tasks
    \item schedule tasks
    \item trap errors
    \item handle errors
    \end{itemize}
\end{itemize}

\subsubsection{Control Supporting Classes}

\begin{description}
\item Task: A Task, which is part of the Parallel component, is the
  combination of a sequence of Method's (algorithms) and a FieldBlock
  (data). Parallel distribution and load-balancing of Tasks is
  controlled by the Parallel component, whereas scheduling of
  inter-Task communication and scheduling of Task methods is
  controlled by the Control component. Since each FieldBlock
  corresponds to a serial thread, a Task is analagous to a "Chare"
  object in CHARM++.
\end{description}

\subsubsection{Control Functions}

\begin{tabular}{ll}
   \code{Control()} 	 & \\
   \code{\~{\ }Control()} 	& \\
    \code{create\_tasks()} & 	Create one or more Tasks \\
    \code{run\_tasks()}    &   Execute one or more Tasks \\
    \code{delete\_tasks()} &   Delete one or more Tasks \\
    \code{push\_tasks()}   &   Push one or more Tasks on a queue \\
    \code{pop\_tasks()}    &   Pop one or more Tasks from a queue
\end{tabular}

\subsubsection{Patch scheduling}

Compared to Enzo, the Control component does not loop through each
level individually; instead, it collapses the dependency tree to its
theoretical shortest path, which is that of the finest level
timesteps. E.g. consider a 1-D problem with two levels of refinement
near the center. The parallel steps for Enzo versus Cello are shown
below, with timesteps moving downward:


The advantage of the Cello approach is that, assuming sufficient
parallelism, the finest level can be advancing continuously. The
disadvantage is that the load is inherently less balanced, since in
Enzo parallel tasks are disjoint (individual levels), compared to
successively inclusive (ranging between "just the finest level" to
"all levels"). In the example, Enzo is perfectly balanced with 2 tasks
active at each step. The Cello approach is never worse: with no
parallelism it is the same as the Enzo approach, and with infinite
parallelism it's twice as fast for this example. However, with a
"practical" amount of parallelism, they will still be close, since
most of the work is at the finest timestep, and that's also the
limiting factor in parallelization.

Alternatively, one could run the entire simulation at the finest
timestep. If sufficient parallelism is available, a simulation would
still run about as fast, and would make much higher utilization of the
parallel hardware (load balancing would be much better).



As an alternative approach, it may be possible to take advantage of
the load imbalance of the "Cello approach", for example by scheduling
I/O tasks on other nodes, or even interleaving/pipelining two
independent simulations(!) Even though independent simulations are
"embarrasingly parallel" and could be implemented as such, this
interleaved approach may potentially greatly increase the efficiency
of using massively parallel architectures.

  

If task scheduling is dynamic, then it may be possible to improve
efficiency by ensuring that finer level tasks that are adjacent to
coarse level tasks are performed first, so that coarse level tasks can
begin earlier, and non-adjacent fine level tasks can run in parallel
with coarser level tasks. Since a bulk of the work is at the fine
level, the main advantage of this would be more to reduce overhead
time between steps, which may be significant for large numbers of
processors. The advantage of running multiple levels at once may be
less advantageous, since the workload at the finest level is about the
same as all other levels combined (for the example).  However, it may
be used to hide latency from communication, and it may help to free up
processor groups faster to improve multitasking (say) I/O with
computation.

As an oversimplified summary:

\begin{itemize}
\item The speed of Cello is theoretically faster than Enzo, by up to a
  factor of two
\item The parallel efficiency of Enzo is theoretically better than
  Cello
\item Uniform time-stepping is preferred when sufficient parallelism
  is available
\item Multiple simulations may be "interleaved" to gain higher
  parallel efficiency than single simulations
\item Intelligent dynamic scheduling might improve efficiency
\end{itemize}
