%0       1         2         3         4         5         6         7         8
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%
%
%
% \subsection{Science Goals}
% \subsection{Software Goals}
% \subsubsection{Adaptive Mesh Refinement}
% \subsubsection{Particles}
% \subsubsection{Hyperbolic and Elliptic Methods}
% \subsubsection{Dynamic Task Scheduling}
%    CHARM++
%
%    large-scale demonstration

%    particle precision
%    grid position
%    timestepping
% REQUIREMENTS: scope of library / framework
%    capabilities
%       physics
%          hyperbolic
%          elliptic
%          ray-tracing?
%       AMR
%       Particles
%       performance
%       parallel scaling
%       data structure scaling
%       ease of use
%       dynamic scheduling
%       aggressive minimization of global communication
%       I/O
%       fault tolerance
%       adaptivity
%       extensibility
% DESIGN: software organization and data structures
%    high-level design
%       OOP
%       components
%          Simulation, Control, Parallel, Task
%          Amr, Patch: stores the AMR hierarchy
%          Fields, Particles, Array: for storing data
%          Block, Fluxes: for computing / communicating data
%          Performance, Error, Parameters, Memory
%             globally-accesible functions; components in
%             aspect-oriented programming paradigm; not sufficiently
%             important to use AspectC, but techniques (primarily
%             using templates) usable
%          Method [Analysis] [Visualize]
%             hooks to user code; multiple methods permitted,
%             selectable and scheduled at run-time
%       user interface
%          External, User
%       application-driven
%    data structures
%
% ALGORITHMS: and how problems solved
%    AMR
%    particles
%    parallelization
%    load balancing
%    fault tolerance
% IMPLEMENTATION
%    Languages: C++, C99, UPC, OpenMP
%    Libraries: HDF5, MPI
%    Frameworks: CHARM++, OpenCL
%    TRAC
%      wiki used for design
%      tickets for tasks and documenting defects
%    Subversion
%    unit testing
%    regression testing
%      lcatest
%      correctness, single-thread performance, weak and strong parallel scaling
%          shallow and deep problems
%      lcaperf
%    software reviews

% Requirements
%    
% Design
% Implementation




%=======================================================================
\documentclass[14pt]{article}
%=======================================================================
\include{include}

\usepackage{natbib}

%=======================================================================

\begin{document}
\Large

\nocite{StSh09} % Scalability challenges for massively parallel {AMR} applications
\nocite{WiHy03} % Enhancing scalability of parallel structured {AMR} calculations
\nocite{GuWi06} % Parallel clustering algorithms for structured {AMR}
\nocite{BuGh08} % Towards adaptive mesh {PDE} simulations on petascale computers

%=======================================================================
\TITLE{A Software Framework for Extreme Adaptive Mesh Refinement}{James Bordner}{$Rev$}
%=======================================================================

% \renewcommand{\pargraph}[1]{}

\section{PROJECT SUMMARY}  \label{s:summary}

\pargraph{Motivation}
%
\pargraph{Proposal}
We propose to develop a new software library for adaptive mesh
refinement (AMR) that allows application developers to develop
multiphysics applications for simulating phenomena on an extreme range
of spacial and temporal scales, and that require an extreme amount of
computation, memory, and storage (petascale and exascale).
%
\pargraph{data structure scope summary}
Our basic data structure is a fully parallel distributed octree, with
associated array and particle data, thus supporting both Eularian, Lagrangian,
and hybrid methods such as Particle Mesh (PM).
%
\pargraph{parallelization strategy}
Multiple parallelization technologies can be used, including hybrid
approaches such as MPI or UPC for distributed memory data, and UPC or
OpenMP for shared memory data.  CHARM++ will also be supported.
Extreme scalability will be obtained by localizing the patch / block
problems as much as is feasible; this will help control scalability
issues related to limited floating-point precision and ranges, as well
as reducing the need for global synchronization and reduction
operations.
%
\pargraph{load balancing}
Dynamic load balancing will be hierarchical, both to better adapt to
the hierarchical nature of extreme scale computational platforms, and
to allow flexibility in load balancing frequency and metrics at different
levels.
%
\pargraph{fault tolerance}
Fault tolerance and software resilience are crucial factors at extreme scales,
since it has been observed that frequency of failures is proportional to
the number of sockets.
due to the reduced MTTI (mean time to interrupt) , since the probability that hardware component, operating system, library, or application component fails Unfortunately, approaches to fault tolerance are still 
%
\pargraph{I/O}
Disk I/O is also a crucial factor for petascale 

\pargraph{availablity}
We plan to make this framework publicly available for scientific research use.

Advantage: designing with extreme scale in mind from the beginning.

% DOE buzzwords
% \url{http://www.er.doe.gov/ASCR/WorkshopsConferences/MathTalks/M\_wright.pdf}

\note{highlight contributions: improved tree-based AMR}
\note{distributed datastructure}
\note{multilevel parallelism}
\note{flexible data storage in memory}
\note{dynamic execution and communication scheduling}

% AMR \\
% applied mathematics\\
% collaborative software environments\\
% computer science technologies\\
% exascale \\
% extreme scale\\
% failure avoidance \\
% failure effects avoidance \\
% fault tolerance\\
% GPU \\
% HPC \\
% memory hierarchy\\
% multicore\\
% multiphysics \\
% multiscale \\
% one-sided communication\\
% performance tuning\\
% petascale \\
% PGAS \\
% research goals\\
% resilient software \\
% UPC \\
% virtual processes \\
% 
% \url{http://diversity.doe.gov/documents/Writing\_Winning\_Proposals.pdf}
% 
% \begin{itemize}
% \item intro: compelling, organization goals and success, relevence to agency
% \item problem statement--needs assessment
%   \begin{itemize}
%     \item purpose
%     \item beneficiaries
%     \item what is being done
%     \item what we will do
%   \end{itemize}
% \item objectives: table with direct items
% \item development plan
%   \begin{itemize}
%    \item chart with timelines
%    \item decision points
%    \item milestones
%    \item tasks
%   \end{itemize}
% \item methods and design
%   \begin{itemize}
%     \item use flow charts and diagrams to break up text
%     \item justify course of action
%     \item highlight innovative features
%   \end{itemize}
% \item past experience
% \item management approach
%   \begin{itemize}
%   \item quality control
%   \item emphasize performance management
%   \item technological systems in place
%   \end{itemize}
% 
%    
% \item preliminary results
% \item good references critical; only give great references
% \item use rfp keywoards
% \item quality control
% \item sustainability--what happens when money ends
% \item deliverables: software, research papers, workshop/training
% sales document not technical manual
% \item visit agency web site for similar funding
% \item make benefits clear
% \item write for evaluators
% \item justify and support all claims
% \item active voice: strong subjects and active verbs
% \item first paragraph sentence conveys topic
% \item 65 characters per line
% \item duplicate information instead of cross reference
% \item use graphics and tables
% \item short text with bullets
% \item stay on topic
% \end{itemize}

\section{EXTREME SCALING ISSUES} \label{s:issues}

%    Hardware-related parallel scaling issues
%        software resiliency
%    AMR-specific parallel scaling issues
%    AMR data structure scaling issues

\section{EXISTING AMR FRAMEWORKS} \label{s:review}

%    AMR types--structured AMR (SAMR-CHOMBO), unstructured AMR (SAMRAI),
%    Paramesh
%    CHOMBO
%    SAMARAI
%    ALPS
%    Gadget-2
\section{CELLO REQUIREMENTS / SCOPE} \label{s:requirements}

%    multi-level
%    multi-physics
%    Enzo II: built on framework, but independent
%    equations
%       hyperbolic conservation
%       elliptic
%       local physics
%    user-supplied functions
%       single-patch advance
%       inter-resolution constraints
%       refinement criteria
%    performance
%       serial performance
%       parallel performance
%       parallel scaling
%       memory usage
%    data structures
%       breadth
%       depth
%    software resiliency
%    hardware support
%       multicore
%       gpu / accelerators
%       distributed and / or shared memory
%    parallel technology support
%       directly: CHARM++, MPI, UPC, OpenMP, + combinations
%       indirectly: GPU / accelerator
%    I/O
%    what we do not do
%       visualization
%       analysis utilities

\section{CELLO DESIGN} \label{s:design}

% Components
%   Method
%   Control, Parallel, Task
%      dependencies known: patch can advance when its neighbor data is known
%   Amr, Array, Patch, Tree, Node 
%   Fields, Particles
%   Block, Fluxes, ItBlock*
%   Memory, Performance
%   Error
%   Monitor, Portal
%   Simulation
%   Parameters
\section{CELLO IMPLEMENTATION} \label{s:implementation}

%   Trac
%   Subversion
%   Languages
%      C++, optional UPC, OpenMP
%   User languages
%      C++, C, Fortran, CUDA
%   Parallel libraries / frameworks
%      optional MPI (one-sided and send-recv), CHARM++
%   testing
%   documentation
%   
\section{DEVELOPMENT PLAN} \label{s:plan}

%   CHARM++ generalized unigrid
%      evaluate effectiveness
%   AMR hyperbolic
%   particle methods
%   AMR elliptic
% 
\section{MILESTONES AND DELIVERABLES} \label{s:milestones}
%
%   Cello software framework
%   Complete Enzo II application
%   large-scale demonstration using Enzo II
%   

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@






%=======================================================================
\section{Extreme Scaling Issues} \label{s:scaling}
%=======================================================================


\begin{verbatim}

load balancing
   hierarchical
   load balance locally in given level
   different metrics at different levels
     memory at node level
     workload below
   different frequency at different levels
     lower frequency balancing at higher levels
        more costly
        solution changes less frequently
     higher frequency balancing at lower levels
        less costly
        solution changes more frequently
   task adjacency maintained
   use collected performance data

dynamic scheduling
   task = grid patch + sequence of 
   dynamically schedule local tasks
      increased scheduling freedom
      improves slack for latency hiding
   schedule communication to/from local patches
   tasks scheduled biased towards patches that limit other patches, e.g. coarse
   optionally pipeline multiple methods on cores
      increase available parallelism: (e.g. deep AMR with many physics modules)
      helpful when multiple physics components available with similar costs
   communication scheduling to maximize releasing patches and prefetch data
   gang-scheduling groups of patches for improved GPU throughput / accelerator utilization

octree AMR versus SAMR   

other AMR libraries / frameworks
   CHOMBO
   Paramesh
   SAMRAI

limitations of existing AMR libraries / frameworks

parallel scalability issues
   global synchronization / reductions
   load balancing
   I/O
   fault tolerance

AMR-specific parallel scalability issues
   elliptic
   remeshing--especially SAMR
   load balancing
   timestep determination

AMR datastructure scalability
   breadth--number of grid patches
   depth--range of multi-resolution
   floating point precision issues
   floating point range issues
   integer range issues

octree advantages
   arguably more scalable

octree limitations
   Morton ordering not always feasible
      different patch sizes
      different particle counts
      adaptive timestepping on finer levels
      physics method variations
         shock capturing--Riemann solver iterations
         variable subcycling of iterative stiff methods

hierarchical parallelism
   encourage communication within hardware components
      sockets within node
      cores within socket
      hyperthreads within core

parallel technology encapsulation / virtualization
  distributed / shared memory
    MPI (two-sided and one-sided) (distributed memory)
    OpenMP (shared memory) 
    UPC (either distributed memory or shared memory)
    CHARM++
  multiple strategies enhance software resiliency
    i.e. buggy MPI implementation--dynamically switch to UPC

hierarchical task scheduling
   multiblock (GPU)
   single block (process)
   plane / line (thread)

hybrid parallel
   MPI + OMP
   MPI + UPC
   UPC + OMP (?)
   CHARM++ + OMP (?)
   flexible subset of cores, sockets, nodes, supernodes
   Task scheduling CHARM++ model, but implemented in MPI, UPC, OMP
   processor-task affinity
software components
   functional decomposition
   ...
user functions supplied
   field / particle initialization
   particle advance
   particle computation
   particle - grid interaction
   block advance (multiple)
   inter-level flux correction
   refine / coarsen
   timestep determination
   constraint enforcement (div B)
   error checking / invariant checking for fault tolerance / software resiliency
      detects memory / cpu errors and mark core as faulty and bypass
relaxed local timestep control
   improves performance by not restricting timestep globally
   adaptive timesteps important for deep AMR runs, e.g. star formation (cite Abel)
   reducing global synchronizations and global dependencies crucial for extreme scaling
   allow variable timesteps within level
      eliminates requirement for global reduction
      timesteps quantized by two
         eliminates roundoff errors
      enforce timestep balancing--temporal level jumps(?)

distributed AMR hierarchy
   local patches: store parent, neighbors, children (sparse)
   not as memory efficient as globally stored nodes (3 pointers, 24 bytes average)
      cite (petascale AMR?)
   proxies for remotely stored patches
task migration
   migrate tasks dynamically
   CHARM++ model: pack, relocate, unpack data
   maintain locality by moving task to owner of a neighbor
   maintain parent-child locality when possible
      relax for deep hierarchies
   migrate using hierarchical parallelism

I/O
   parallel HDF5
   compression
   CRC error-checking and retry
   subset of nodes do I/O
   detection of faulty disk and mark as unusable
   different formats for different uses

fault-tolerance / software resilience strategies
   need to deal with continuous stream of failures
   MTTF < MTTC
   checkpoint to disk
      issue: failures will become more frequent than time to checkpoint
      agressively reduce checkpoint data size and write time
         dedicated I/O nodes
         compress
         check data
         methods identify which data modified
            may help lower disk output--only checkpoint modified data
   checkpoint to memory
     CHARM++ does this(?)
   detect hardware errors and mark as defective
       memory
       disk
       core
       socket
       node
       interconnect (pairs of nodes)
       software libraries (MPI versus UPC, etc.)
   flash memory
   log faults to disk for subsequent analysis
   performance resilience
      dynamically adapt to reduce cache thrashing / ineffeciency
          array blocking or padding in computational array registers
      adapt AMR patch size, refinement factor (2,4,8)
   fault-tolerant MPI
      FT-MPI
   leverage new approaches when available
      active research area
      keep up to date in latest practices
      design software to use new approaches

memory storage formats and data movement
   different formats for different uses
        optimized for storage
          option to not store guard / ghost cells
          AMR patches may be larger than computationally optimal
        optimized for computing
          store ghost cells
          improved data locality
          compute block size optimized for cache
             copied to array compute registers
             blocking/padding for cache / memory hierarchy
             groups of blocks for GPU / fp accelerators
   compute in reformatted ``computational registers''
      basic idea used in Paramesh
         encapsulates functionality of both Paramesh ``working block'' and ``work'' datastructures
      more flexible:
        arbitrary reordering of array axes
        arbitrary subset of axes
        arbitrary ghost zone depth
          more adaptable to existing legacy ``unigrid'' routines
          different reorderings possible for different methods (hydro, chemistry, etc.)
          flexibilty allows improved data-layout for single-thread performance
             cache blocking, padding
   communicate using reformatted ``flux registers''
      idea used in CHOMBO
   scheduler reuse for computation / communication scheduling

patch-local coordinates for particles
   control accumulation of roundoff
   minimize / eliminate effects of limited precision and range
   absolute coordinates computed when necessary
      analysis
      visualization

relative coordinates for patches   
   relative to neighbors
   computations frequently only require cell size and timestep
      hydrodynamics
   absolute coordinates computed dynamically when necessary to required precision
      initialization
      inline analysis
      global position dependent physics (materials)

global scaling of dx, dt?
   may not be needed
      only power-of-two quantization of dx, dt
        mantissa unchanged--no fp precision issue
        fp range issue
            support coordinate rescaling at deep levels
        
visualization
   use existing library, e.g. Visit
   Method can include visualization or inline analysis

software resilience
   take advantage of only Methods change data
   methods signal which fields / particles changed

particles
   groups of related particles stored together
   particles associated with grid patch
   flexibility in particle ownership
     may belong to immediate neighbor to reduce communication

AMR algorithm enhancements: patch coalescing
   improves ``shallow'' AMR
   SAMR does this naturally--tree-based AMR doesn't
   especially helpful when large connected regions of domain at finest level
   less AMR overhead: fewer nodes / patches
   task size still controlable through computational registers
   EXAMPLE: cosmology

AMR algorithm enhancements: targeted refinement by 4 or 8
   improves ``deep'' AMR
   refinement by 2 too ``shallow''
   back filling possible to regain effect of balanced tree
   sparse storage format of child patches to reduce storage 
   balancing octrees can have global effect
     balancing refinementy by 4 or 8 probabalistically (provably?) local
     no explicit AMR patches required for backfill--can live on child nodes 
     EXAMPLE: point refinement

problem scope
  hyperbolic conservation
     hydrodynamics
  elliptic problems
     self-gravity
  local physics
     chemistry
  elliptic support:
      global and level solves

AMR hierarchy data
     fields at multiple refinement levels (internal tree nodes as well as leaves)
       SEE Paramesh user guide for motivation
       improving solution
       refinement criteria (cite Berger)
     fields only finest level (only tree leaves)
       reduced memory usage
       reduced computation: only finest level
       reduced communication: no inter-level interpolation (except refinement)

monitor: support for interfacing with user while running
   ``dashboard'' for real-time monitoring state of simulation
   
portal: support interfacing with other codes
   post-processing solvers
   data analysis pipeline
   visualization pipeline

automatic dynamic tuning of parameters
   patch size (by level)(possible?)
   refinement factor (2,4,8)
   load balancing frequency

application driven
   Enzo II
   cosmology / astrophysics
   requires wide range of AMR capabilities
     broad for galaxy structure formation
     deep for star formation
     turbulence
   requires wide range of physics capabilities
     hyperbolic: hydrodynamics
     elliptic: self-gravity, FLD radiation
     local physics: chemistry, heating/cooling
   decouple physics modules from AMR framework
   make both Enzo II and underlying AMR framework publicly available

testing
   lcatest: automated parallel application testing
   multiple test levels
      unit tests
      component tests
      application tests
      in-house / community beta-testing
         (progressive as functionality comes online)
   test for multiple things:
      functionality
      correctness
      performance
      scaling
   tests also help supplement user documentation
   use integrated performance monitoring
      PAPI for hardware counters
      PMPI for MPI communication
      new[] / delete[] overload for dynamic memory usage
         particularly important for AMR
      user-defined independent attributes
         cycle
         level
         process
      user-defined dependent metrics
         process-local patch counts
         process-local cell / zone counts
         process-local particle counts
      less frequent output at finer levels (more data)
   helps identify functional, performance, scaling bugs early
   extreme scaling designed into framework from the start

integrated performance monitoring
   summaries at different hardware levels
   less frequent at lower levels--more data
   more frequent at upper levels
   performance data available to other components
      load balancing based on actual memory usage / cpu time
      feedback for adaptivity
      help identify performance and scaling issues early
      poor-performance resilient

design
   still primarily in design phase
      available flexibility to modify current design
      little code and time wasted
   emphasis on thorough designing before coding
      prototype ideas before final implementation
      more time upfront known to lead to faster overall development times

upcoming systems
   BW
   ...
   ...
   core counts
   extrapolate top500 10 years
   what this means for extreme scalable AMR + particles

AMR basic unigrid
   unigrid is single AMR node(?)
     (how to link with level 1?)

I/O different output formats for different uses
   checkpointing
   analysis
   visualization
   general ``data dump''
   cheaper to rerun and regenerate data to process than dump and reread later
      inline analysis capability to reduce overall output required
   checkpoints
       node / processor independent: software resiliency
       checkpoints restartable on different configurations / platforms
   ``accessor code'' included with all output data

power -- idle components when not in use to reduce power usage
   shared caches help normalize performance

development
   implement prorgessively to fill user beta testing pipeline

\end{verbatim}

Petascale AMR research issue.


\pargraph{equation scope}
Nonlinear time-dependent hyperbolic conservation laws, and elliptic PDE's

\begin{itemize}
\item synchronization and collective operations
\item parallel I/O
\item fault tolerance / software resiliency
\item distributed data structures
\item hierarchical memory
\item hierarchy parallelism
\item dynamic load balancing
\item heterogeneous computational components: GPU's
\item ease of use: unigrid update + interpolation + flux correction
\item weigh advantages / disadvantages of patch- versus tree-amr
\end{itemize}


\section{Review of Existing AMR frameworks}

\pargraph{CHOMBO}

  Block-structured AMR

\pargraph{SAMRAI}

   [2001]
   good scaling for numerical and data communication
   poorer scaling in adaptive meshing and communication schedule construction 

\pargraph{Paramesh}

  Octree-based AMR

GADGET-2: Preeminent cosmological TreeSPH simulation code
DAGH (1998)

\section{Project requirements}

\section{Project design}

General goals

\begin{itemize}
\item extreme parallel scalability
\item extreme data structure scalability
  \begin{itemize}
  \item breadth: number of grid patches
  \item depth: number of AMR hierarchy levels
  \end{itemize}
\end{itemize}

\begin{itemize}
\item for parallel scalability: aggressive minimization of
  synchronization and collective operations
\item ``localize'' problem as much as possible
  \begin{itemize}
  \item For each patch, only store patch, location information for
    neighboring patches (``ghost patches''), and ancestors for octree
  \item patch-local adaptive timestep control
    \begin{itemize}
    \item prevents rapidly evolving feature in one localized region of
      domain from determining timestep elsewhere
    \end{itemize}
  \end{itemize}
\item facilitate dynamic load balancing, dynamic scheduling of
  communication and computation
\item maximize data structure flexibility
\item dynamically optimize data structure parameters to hardware
  during run time
\item for algorithmic scalability: aggresively minimize dependencies
  of fp and integer precision on data structure size
\item related to ``localization'' of tasks
  \begin{itemize}
  \item particle positions stored using local coordinate system with
    origin at patch corner
  \item absolute patch coordinates not stored, only position relative
    to neighbors
     \begin{itemize}
     \item global coordinates only needed for initialization
     \item only need mesh width and timestep size for advancing
     \item not needed for restarts
     \item can still be computed dynamically
     \item localization an option--not needed for smaller problems
     \item absolute particle positions / grid locations may be needed
       for some output, e.g. global particle analysis
     \item in that case low precision
     \item still scaling issues relative to depth
      \begin{itemize}
      \item neighbor location
      \end{itemize}
     \end{itemize}
  \end{itemize}
\end{itemize}

\begin{itemize}
\item targeted refinement by 4 or 8
  \begin{itemize}
  \item especially effective for deep AMR, e.g. star formation with
    30+ levels
  \item still supports effective resolution jumps of 2 using backfill
  \end{itemize}
\item patch coalescing
  \begin{itemize}
  \item reduced AMR overhead (number of patches) especially in shallow
    AMR relative to Paramesh
  \item improved performance since locally unifrom grid instead of AMR
    patches
  \item still supports variable block sizes for optimizing memory
    hierarchy, parallel task size, task counts
  \end{itemize}
\item variable patch resolution
\item instead of uniformly refining a patch, can replace with higher
  resolution array
\item need AMR machinery where resolution requirements change, not
  where high resolution is needed
\end{itemize}


\section{Project data structures and algorithms}

Amr, Tree, Array, Patch

\begin{center}
\begin{minipage}{7.0in}
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{cosmo2.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{cosmo2-4-1.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{cosmo2-4-2.png}
\end{minipage}
\end{minipage} \\
\ \\
\begin{minipage}{7in}
\textbf{Left}: 2D Source cosmology density field projection.  
\textbf{Middle}: Balanced octree: 81701 patches.
\textbf{Right}: Balanced octree with coalesced patches: 32529 patches.
\ \\
\end{minipage}
\end{center}


\begin{center}
\begin{minipage}{7.0in}
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{dots.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{dots-4-1.png}
\end{minipage} \ 
\begin{minipage}{2.2in}
\includegraphics[width=2.2in]{dots-16-5.png}
\end{minipage}
\end{minipage} \\
\begin{minipage}{7in}
\begin{minipage}[t]{2.2in}
2D Source cosmology density field projection
\end{minipage} \ 
\begin{minipage}[t]{2.2in}
Balanced octree: 2137 patches
\end{minipage} \ 
\begin{minipage}[t]{2.2in}
Balanced sparse $4^3$-tree: 158 explicit patches
\end{minipage}
\end{minipage}
\end{center}

\begin{itemize}
\item generalized octree-based AMR 
  \begin{itemize}
  \item 8-tree, sparse 64-tree, sparse 512-tree
  \item decoupled AMR / Array
  \item 
  \end{itemize}
\item Fully distributed AMR data structure
  \begin{itemize}
\item For each local grid patch, only immediate neighbors and
  ancestors stored
  \item Proxies for remotely stored patches
    \begin{itemize}
    \item remote thread identifier + pointer
    \end{itemize}
  \end{itemize}
\item Store only actual data
  \begin{itemize}
  \item use flux registers for communication
  \item allocate patch ghost zones only for computation
  \end{itemize}
\item Could store appropriate subset of neighbor data as well for
  fault-tolerance
\item AMR operations
  \begin{itemize}
  \item initial grid generation
  \item refinement criteria: tag refine or coarsen
  \item refinement or coarsening localized
  \end{itemize}
\end{itemize}


\section{Project Implementation}

\subsection{I/O}

\begin{itemize}
\item Routines for data dumps, checkpointing, output of visualizaton /
  analysis
\item Leverage standard for AMR data output when possible
\item parallel HDF5
\item support I/O from subset of nodes , dedicated or shared, to
  optimize I/O performance, and overlap I/O with computation
\end{itemize}

\subsection{Parallelism}
\begin{itemize}
\item CHARM++ to control data placement, task scheduling,
  checkpointing for fault tolerance
\item platform hierarchical architecture-aware data structures
  \begin{itemize}
  \item e.g. MPI communicator for cores in a socket, sockets in a
    node, nodes in a supernode, supernodes in a machine
  \item facilitates hierarchical dynamic load balancing
    \begin{itemize}
    \item improves dynamic mapping of data structures to hardware components
    \item E.g. load balance more frequently at core / socket level to
      keep functional units busy
    \item load balance node / supernode levels less frequently to keep
      memory usage uniform
    \item less frequent because:
      \begin{itemize}
      \item problem changes less at larger scales
      \item rebalancing is more expensive: larger data sizes, slower
        interconnects
      \end{itemize}
    \item user-defined parameters and metrics for load balancing at
      different levels
      \begin{itemize}
      \item dynamically collected performance data can be fed back
        into hierarchical load balancing algorithm
      \end{itemize}
    \end{itemize}
  \item note linearization of octree datastructure is insufficient:
    assumes equal work per patch
    \begin{itemize}
    \item particles are associated with nodes, changing weight
    \item adaptive timestepping drastically weights more highly
      refined patches
    \item performance of physics algorithms on a patch is not
      necessarily uniform, e.g. localized chemistry subcycling, front
      tracking, etc.
    \item arrays on patches may be different sized
    \item linearization of patches reduces flexibility, and constrains data movement to a single dimension
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Fault tolerance}
\begin{itemize}
\item  FT-MPI 
  \begin{itemize}
  \item ``fault-tolerante MPI''
  \item http://icl.cs.utk.edu/ftmpi/overview/index.html 
  \end{itemize}
\item MPICH-V 
  \begin{itemize}
  \item ``MPI Implementation for Volatile resources''
  \item http://mpich-v.lri.fr/index.php 
  \end{itemize}
\end{itemize}

\subsection{Software implementation}

\begin{itemize}
\item Trac + SVN or mercurial
\item documenting as I go
\item OOP design: improves component reuse, controls software
  complexity, eases software maintenance
\item heavy emphasis on designing before coding
\end{itemize}
\begin{itemize}
\item organize into components (subdirectories) of related classes
  \begin{itemize}
  \item Simulation, Control, Parallel, Task
  \item Amr, Patch: stores the AMR hierarchy
  \item Fields, Particles, Array: for storing data
  \item Block, Fluxes: for computing / communicating data
  \item Performance, Error, Parameters, Memory: globally-accesible functions; components in aspect-oriented programming paradigm; not sufficiently important to use AspectC, but techniques (primarily using templates) usable
  \item Method [Analysis] [Visualize]: hooks to user code; multiple methods permitted, selectable and scheduled at run-time
  \item External, User
  \end{itemize}
\end{itemize}
\begin{itemize}
\item interdependencies controlled at component and class level
\item classes with access functions used instead of raw arrays to improve modularity and modifiability
\item block structure for interface with user code
\item unit testing, including performance and parallel scaling
\item prototyping for proof-of-concept
\item application testing
\item code reviews
\end{itemize}

\begin{itemize}
\item Languages C++; user code can be C, C++, Fortran
\item attempt to componentize parallelization : MPI (two-sided, one-sided) + OMP + UPC + GPU + CHARM++
\item advantages / disadvantages of each
  \begin{itemize}
  \item MPI: + widespread, optimized implementations, familiar
  \item MPI: - data replication, difficult to use
  \item UPC + easier to use, combines shared memory view with efficient data affinity
  \item UPC - no concept of MPI communicator, still under development--not as mature
  \item OMP + can be used progressively
  \item OMP - not scalable outside of socket / node;  inefficiencies due to false cache sharing
  \item GPU + very fast / power efficient when usable
  \item GPU - no usable standard, difficult to program, difficult to map problem to hardware
  \item CHARM++ + higher-level, dynamic scheduling, dynamic load-balancing, fault tolerant through checkpointing to other node memory
  \item CHARM++ - requires learning separate language, separate runtime system, no data prefetching(?)
  \end{itemize}
    \begin{itemize}
    \item currently not fully realizable for GPU since depends on
      computational code
    \item hierarchical parallelism: MPI + OMP, MPI + UPC, MPI + GPU,
      etc.
    \item advantages of hybrid
      \begin{itemize}
      \item reduced data replication from MPI distributed memory
      \item dynamic parallel threads--use more when helpful, fewer when not
      \item UPC
      \end{itemize}
    \item disadvantages of hybrid
      \begin{itemize}
      \item performance hit from data sharing in MPI + OMP
      \item MPI and UPC communication cannot (currently) proceed concurrently
      \end{itemize}
    \item code for two modes: distributed memory and shared memory
    \item parallel tasks: grid patches, arrays, grid patch groups,
      particle groups
    \item flexible data structure parameters (grid patch size, patch
      decomposition, patch grouping) to dynamically optimize task size
    \end{itemize}
\end{itemize}
    
%=======================================================================
\section{Enhancements}
%=======================================================================

\begin{itemize}
\item Require more rigorous coding standards compared to Enzo
  development
\item Auto-tune where possible--automatically optimize for cache-,
  parallel-, vector-, solver-, etc. parameters
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Adaptive Mesh Refinement}
%-----------------------------------------------------------------------

\begin{itemize}
\item enhancements for both ``deep'' and ``shallow'' AMR
\end{itemize}

\begin{itemize}
\item Do not store a patch's global position, only local position
  relative to immediate neighbors, parent, and children. [Toward
  distributed AMR data-structure, and to address precision issues with
  deep AMR. Potential issues: boundary and initial conditions.] (see
  W013)
\item For very deep AMR where coarser levels never complete their
  timestep, delete coarse levels to free storage.
\item  Provide (or notify) neighboring patches with updated
  ghost zone data as soon as it's available.
\item  Support temporary ``allocate as-needed'' ghost zones
  in addition to ``permanent'' ones
\item Reduce tree-AMR node size by only storing parent, single
  neighbor, and single child. [Assuming one pointer for field data and
  SAMR patches indexed by single-precision offsets into parent
  patches, 32 bytes / tree-AMR and 48 bytes / patch-AMR]
\item Relax rigid refinement criteria to inhibit excessive changes in
  octree-like tree refinement.
\item Represent patch extents with (small) integer values relative to
  parent. [To reduce memory usage with deep AMR runs.] (see W012)
\item Support both structured AMR (Enzo-like) and tree-based AMR
\item Support flexible node types: memory-efficient versus
  compute-efficient.
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Arrays}
%-----------------------------------------------------------------------

\begin{itemize}
\item Allow multiple root-level patches per MPI task. [To improve
  cache use for unigrid problems, and improve load-balancing for AMR.]
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Control}
%-----------------------------------------------------------------------
\begin{itemize}
\item Reduce implicit dependencies by dynamically allocating parallel
  tasks, ala CHARM++.(e.g. currently Enzo loops through patches within
  a level, but a given patch can proceed as soon as it has all its
  boundary data)
\item Support optional uniform timesteps across all levels. [To
  improve parallel efficiency.]
\item Support optional variable timestep sizes within each level. [To
  reduce synchronization costs when computing global CFL condition.]
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Storage}
%-----------------------------------------------------------------------
\begin{itemize}
\item Enforce strict control over data storage formats (e.g. files)
  (see W0009)
\item Require that all stored data be accessed through standard
  interface functions that are independent of specific file formats
  (i.e., stored datasets are conceptually treated as objects)
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Fields}
%-----------------------------------------------------------------------
\begin{itemize}
\item User-controlled optional floor/ceiling limits on individual
  Fields (ala ``tiny\_number'' in Enzo), with user-specified Error
  behavior (warning, error, ignore, reset to given floor/ceiling,
  etc.)
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Methods}
%-----------------------------------------------------------------------
\begin{itemize}
\item Integrate ``inits'' functionality into the main code
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Parallelization}
%-----------------------------------------------------------------------
\begin{itemize}
\item Load-balance by having over-loaded processors reassign tasks to
  random processes. [To reduce global communication for determining
  which processes are under-loaded]
\item Load-balance using ``over-compensation'', since heavily-loaded
  processes tend to continue to become more heavily loaded (cosmology
  / star-formation application-dependent).
\item Only use inter-core, inter-cpu, inter-node, etc.
  level-communicators to bound communicator size and manage
  communication nonuniformity.
\item Support multiple (hybrid) and flexible parallelization
  strategies, including MPI-1 (2-sided send/recv), MPI-2 (1-sided
  get/put), OMP, and optionally UPC and GPU.
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Parameters}
%-----------------------------------------------------------------------
\begin{itemize}
\item Support for user-supplied code for problem initialization.
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Particles}
%-----------------------------------------------------------------------
\begin{itemize}
\item Store particle positions in single precision as -1 <= x,y,z <= 1
  relative to their containing patch. [To reduce storage, improve
  performance, and address precision issues with deep AMR.]
\item Use a binary tree data-structure to recursively partition the
  bounding boxes of particles.
\end{itemize}

%-----------------------------------------------------------------------
\subsection{Simulations}
%-----------------------------------------------------------------------
\begin{itemize}
\item Support ensembles within a single run, including inline-analysis
\end{itemize}


%=======================================================================
\bibliography{papers}
\bibliographystyle{unsrt}
%=======================================================================

\end{document}

%==================================================================

